{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QeMSuCz44m6e"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Path"
      ],
      "metadata": {
        "id": "QeMSuCz44m6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global path\n",
        "GLOBAL_PATH = \"/content/drive/MyDrive/CS433\"\n",
        "\n",
        "# GloVe\n",
        "GLOVE_PATH = f\"{GLOBAL_PATH}/data/glove.twitter.27B.100d.txt\"\n",
        "\n",
        "# Train full\n",
        "TRAIN_NEG_FULL_PATH = f\"{GLOBAL_PATH}/data/train_neg_full.txt\"\n",
        "TRAIN_POS_FULL_PATH = f\"{GLOBAL_PATH}/data/train_pos_full.txt\"\n",
        "\n",
        "# Train\n",
        "TRAIN_NEG_PATH = f\"{GLOBAL_PATH}/data/train_neg.txt\"\n",
        "TRAIN_POS_PATH = f\"{GLOBAL_PATH}/data/train_pos.txt\"\n",
        "\n",
        "# Test\n",
        "TEST_PATH = f\"{GLOBAL_PATH}/data/test_data.txt\"\n",
        "\n",
        "# Preprocessed data\n",
        "TRAIN_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/train_gru.csv\"\n",
        "TEST_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/test_gru.csv\"\n",
        "\n",
        "# Weight\n",
        "WEIGHT_PATH = f\"{GLOBAL_PATH}/weight\""
      ],
      "metadata": {
        "id": "2M_P_rIU4l1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rGFc0ZYjtIrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Ey7Y3WPJ5K2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Iw6ETtm6Vbo",
        "outputId": "69415960-967f-4ff5-c2f2-25fddb364766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: editdistpy\n",
            "  Building wheel for editdistpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl size=187460 sha256=78ecfbfeeb9df2ca63154b84ec16b1cbfcb9daf21e7e7b11822c8f1008f6ef7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/6a/a6/a1283cc145323a1fb3d475bd158ee60b248ab1985230d266fc\n",
            "Successfully built editdistpy\n",
            "Installing collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.3 symspellpy-6.7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from symspellpy import SymSpell # Fuzzy search and word correction"
      ],
      "metadata": {
        "id": "UXKwzkeE6SrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMOTICONS_GLOVE = {\n",
        "  '<smile>': [':-]', '0:3', '8)', \":'-)\", '>^_^<', '(^_^)', \"(';')\", ':*',\n",
        "    '(^^)/', ':)', ':>', '(*_*)', '(^^)v', '=3', ':}', ';^)', ':->', '^_^;',\n",
        "    '=)', '(^o^)', '*)', '(^.^)', '^_^', '\\\\o/', '^5', '(__)', '(#^.^#)', '0:)',\n",
        "    '(^^)', ';]', ':-*', ':^)', ':3', '(+_+)', ';)', \":')\", '(:', ':-3', ':-}',\n",
        "    ';-)', ':-)', ':]', '*-)', 'o/\\\\o', '=]', '(^_-)', '8-)', ':o)', ':c)',\n",
        "    '(^_^)/', '(o.o)', ':o', '>:)', '8-0', ':-0', ';3', '>:3', '3:)', ':-o',\n",
        "    '}:)', 'o_0', '^^;', 'xx', 'xxx', '^o^', ':d', ' c:'],\n",
        "  '<lolface>': [':-p', ':p', ':b', ':-b', 'x-p', '=p'],\n",
        "  '<heart>': ['<3'],\n",
        "  '<neutralface>': ['=\\\\', '>:/', '(..)', '(._.)', ':-/', ':|', '>.<', ':-.',\n",
        "    \"('_')\", '=/', ':/', ':#', '(-_-)', 'o-o', 'o_o', ':$', '>:\\\\', ':@', ':-|',\n",
        "    '><>', '(-.-)', ':\\\\', '<+', ':-@'],\n",
        "  '<sadface>': [';(', '(~_~)', ':c', ':[', ':-&', ':(', '>:[', ':&', ':-c',\n",
        "    ';n;', \":'(\", ';;', ':-[', ';-;', '%)', ':<', '<\\\\3', ':{', ';_;', '=(',\n",
        "    'v.v', 'm(__)m', '</3', \":'-(\", ':-<']\n",
        "}\n"
      ],
      "metadata": {
        "id": "uDhvfDBk5Nwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk weights\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDstAhJJ6h7z",
        "outputId": "62159645-f6f0-4442-8a8b-e26a93367c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def function_name(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print(f\"Executing function: `{func.__name__}`\")\n",
        "        return func(*args, **kwargs)\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "B5sXEg1IIZxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, path_ls: list[str], is_submission: bool = False) -> None:\n",
        "        if is_submission:\n",
        "            with open(path_ls[0]) as f:\n",
        "                content = f.read().splitlines()\n",
        "\n",
        "            ids = [line.split(\",\")[0] for line in content]\n",
        "            text = [\",\".join(line.split(\",\")[1:]) for line in content]\n",
        "\n",
        "            self.__df = pd.DataFrame({\"ids\": ids, \"text\": text})\n",
        "        else:\n",
        "            # Initiate df from the train data\n",
        "            self.__df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "            for i, path in enumerate(path_ls):\n",
        "                with open(path) as f:\n",
        "                    content = f.read().splitlines()\n",
        "\n",
        "                __temp_df = pd.DataFrame({\"text\": content, \"label\": np.ones(len(content)) * i})\n",
        "\n",
        "                self.__df = self.__df.append(__temp_df).reset_index(drop=True)\n",
        "\n",
        "            self.__df[\"raw_text\"] = self.__df[\"text\"]\n",
        "\n",
        "\n",
        "    def get(self) -> pd.DataFrame:\n",
        "        return self.__df\n",
        "\n",
        "    @function_name\n",
        "    def lower(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.lower()\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def drop_duplicates(self):\n",
        "        self.__df = self.__df.drop_duplicates(subset=[\"text\"])\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_tags(self):\n",
        "        # Remove <user> and <url> and ...\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(\"<[\\w]*>\", \"\")\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].apply(lambda x: x.strip())\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(\"\\.{3}$\", \"\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_number(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(\"\\d\", \"\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_punctuation(self):\n",
        "        self.__df[\"text\"] = str(self.__data[\"text\"]).replace(\"[^\\w\\s]\", \"\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_stopword(self):\n",
        "        stopword_list = set(stopwords.words(\"english\"))\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].apply(lambda text: \" \".join([word for word in str(text).split() if word not in stopword_list]))\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_elong(self):\n",
        "        elf.__df[\"text\"] = self.__df[\"text\"].apply(lambda x: str(re.sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2\", x)))\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_whitespace(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(\"\\s{2,}\", \" \")\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].apply(lambda x: x.strip())\n",
        "        self.__df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def remove_space_between_emoticons(self):\n",
        "\n",
        "        # Getting list of all emoticons\n",
        "        emo_list = [el for value in list(EMOTICONS_GLOVE.values()) for el in value]\n",
        "\n",
        "        # Putting a space between each character in each emoticon\n",
        "        emo_with_spaces = \"|\".join(re.escape(\" \".join(emo)) for emo in emo_list)\n",
        "\n",
        "        # Getting all emoticons that don\"t contain any alphanumeric character\n",
        "        all_non_alpha_emo = \"|\".join(re.escape(emo) for emo in emo_list if not any(char.isalpha() or char.isdigit() for char in emo))\n",
        "\n",
        "        # Removing spaces between emoticons\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(emo_with_spaces, lambda t: t.group().replace(' ', ''))\n",
        "\n",
        "        # Adding space between a word and an emoticon\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(rf\"({all_non_alpha_emo})\", r\" \\1 \")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def reconstruct_ending_emoticon(self):\n",
        "        # Reconstruct emoticon at the end of line\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(\"\\)+$\", \":)\").replace(\"\\(+$\", \":(\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def emoticons_to_tags(self):\n",
        "        # Dictionary like {tag:[list_of_emoticons]}\n",
        "        union_re = {}\n",
        "        for tag, emo_list in EMOTICONS_GLOVE.items():\n",
        "            # Getting emoticons as they are\n",
        "            re_emo = \"|\".join(re.escape(emo) for emo in emo_list)\n",
        "            union_re[tag] = re_emo\n",
        "\n",
        "        # Function to be called for each tweet\n",
        "        def _inner(text, _union_re):\n",
        "            for tag, union_re in _union_re.items():\n",
        "                text = re.sub(union_re, \" \" + tag + \" \", text)\n",
        "            return text\n",
        "\n",
        "        # Applying for each tweet\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].apply(lambda x: _inner(str(x), union_re))\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def final_emoticons_to_tag(self):\n",
        "        # Reconstruct emoticon at the end of line\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].str.replace(\"\\)+$\", \"<smile>\").replace(\"\\(+$\", \"<sadface>\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def hashtags_to_tags(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].replace(r\"#(\\S+)\", r\"<hashtag> \\1\")\n",
        "\n",
        "    @function_name\n",
        "    def numbers_to_tags(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].replace(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", r\"<number>\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def repeat_to_tags(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].replace(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
        "\n",
        "\n",
        "    @function_name\n",
        "    def elongs_to_tags(self):\n",
        "        self.__df[\"text\"] = self.__df[\"text\"].replace(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n"
      ],
      "metadata": {
        "id": "sJPfynso6wuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewPreprocessing:\n",
        "    \"\"\"\n",
        "    Preprocesses the data and can even perform feature extraction.\n",
        "\n",
        "    Attributes:\n",
        "      __data: A pandas dataframe with the data (at least one column called text).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, list_: list, submission=False):\n",
        "        \"\"\"\n",
        "        Builds the Pandas DataFrame.\n",
        "          - If submission is False, a list of 2 elements is expected.\n",
        "            The first one must be the negative tweets.\n",
        "            The second one must be the positive tweets.\n",
        "            The final DataFrame is composed of `text` and `label` columns.\n",
        "          - If submission is True, a list of 1 element is expected.\n",
        "            The final DataFrame is composed of `ids` and `text` columns.\n",
        "\n",
        "        :param list_: a list of .txt files to be converted in DataFrame\n",
        "        :type list_: list\n",
        "        :param submission: specify the type of DataFrame (train or test data)\n",
        "        :rtype submission: bool\n",
        "        \"\"\"\n",
        "\n",
        "        if not submission:\n",
        "            if len(list_) == 2:\n",
        "\n",
        "                # Creating empty DataFrame\n",
        "                self.__data = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "                # Reading the content of each file in the list\n",
        "                for i, file_name in enumerate(list_):\n",
        "                    with open(file_name) as f:\n",
        "                        content = f.read().splitlines()\n",
        "\n",
        "                    # Creating a DataFrame putting as label the position in the input list\n",
        "                    df = pd.DataFrame(\n",
        "                        columns=[\"text\", \"label\"],\n",
        "                        data={\"text\": content, \"label\": np.ones(len(content)) * i},\n",
        "                    )\n",
        "\n",
        "                    # Appending the dataframe\n",
        "                    self.__data = self.__data.append(df).reset_index(drop=True)\n",
        "\n",
        "        else:\n",
        "            if len(list_) == 1:\n",
        "                # Reading the content\n",
        "                with open(list_[0]) as f:\n",
        "                    content = f.read().splitlines()\n",
        "\n",
        "                # Getting the ids\n",
        "                ids = [line.split(\",\")[0] for line in content]\n",
        "                # Getting the tweets' content\n",
        "                texts = [\",\".join(line.split(\",\")[1:]) for line in content]\n",
        "\n",
        "                # Creating the DataFrame\n",
        "                self.__data = pd.DataFrame(\n",
        "                    columns=[\"ids\", \"text\"], data={\"ids\": ids, \"text\": texts}\n",
        "                )\n",
        "\n",
        "    # UTILITY METHODS\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Returns the DataFrame.\n",
        "\n",
        "        :return: the DataFrame\n",
        "        :rtype: pandas.DataFrame\n",
        "        \"\"\"\n",
        "        return self.__data\n",
        "\n",
        "    def logging(self):\n",
        "        \"\"\"\n",
        "        Prints the first 10 rows in the dataframe stored in self.__data.\n",
        "        \"\"\"\n",
        "        print(\"Logging:\")\n",
        "        print(self.__data[\"text\"].head(10))\n",
        "\n",
        "    def save_raw(self):\n",
        "        \"\"\"\n",
        "        Creates a column in the dataframe as copy of `text` column\n",
        "          to keep the original data.\n",
        "\n",
        "        Must be called before anything else!\n",
        "        \"\"\"\n",
        "        print(\"Saving raw tweet...\")\n",
        "\n",
        "        self.__data[\"raw\"] = self.__data[\"text\"]\n",
        "\n",
        "    # PREPROCESSING METHODS\n",
        "\n",
        "    def drop_duplicates(self):\n",
        "        \"\"\"\n",
        "        Removes duplicated in the dataframe according to text column.\n",
        "        \"\"\"\n",
        "        print(\"Dropping duplicates...\")\n",
        "\n",
        "        self.__data = self.__data.drop_duplicates(subset=[\"text\"])\n",
        "\n",
        "    def remove_tags(self):\n",
        "        \"\"\"\n",
        "        Removes tags (<user>, <url>) and final '...' characters (long tweets)\n",
        "        \"\"\"\n",
        "        print(\"Removing tags...\")\n",
        "\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"<[\\w]*>\", \"\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(lambda text: text.strip())\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\.{3}$\", \"\")\n",
        "\n",
        "    def convert_hashtags(self):\n",
        "        \"\"\"\n",
        "        Removes '#' at the beginning of a tweet and corrects spacing of it.\n",
        "        \"\"\"\n",
        "        print(\"Converting hashtags...\")\n",
        "\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            \"(#)(\\w+)\",\n",
        "            lambda text: Preprocessing.__word_segmentation(str(text.group(2))),\n",
        "        )\n",
        "\n",
        "    def slangs_to_words(self):\n",
        "        \"\"\"\n",
        "        Extends slangs to sequence of words.\n",
        "        \"\"\"\n",
        "        print(\"Converting slangs to words...\")\n",
        "\n",
        "        # Reading the slangs from file\n",
        "        with open(\"./utility/slang.txt\") as f:\n",
        "            chat_words_str = f.read().splitlines()\n",
        "\n",
        "        # List of mappings {slang: slang_expanded}\n",
        "        chat_words_map_dict = {}\n",
        "\n",
        "        # List of slangs\n",
        "        chat_words_list = []\n",
        "\n",
        "        for line in chat_words_str:\n",
        "            # Slang\n",
        "            cw = line.split(\"=\")[0]\n",
        "\n",
        "            # Slang expanded\n",
        "            cw_expanded = line.split(\"=\")[1]\n",
        "\n",
        "            # Appending slang and mapping\n",
        "            chat_words_list.append(cw)\n",
        "            chat_words_map_dict[cw] = cw_expanded\n",
        "\n",
        "        # Make sure slangs in list are unique\n",
        "        chat_words_list = set(chat_words_list)\n",
        "\n",
        "        # Function to be called for each tweet\n",
        "        def chat_words_conversion(text):\n",
        "            new_text = []\n",
        "\n",
        "            # For each word in the tweet\n",
        "            for w in text.split():\n",
        "\n",
        "                # If slangs is in the mapping\n",
        "                if w.upper() in chat_words_list:\n",
        "                    new_text.append(chat_words_map_dict[w.upper()])\n",
        "\n",
        "                # Otherwise, use the slang itself\n",
        "                else:\n",
        "                    new_text.append(w)\n",
        "            return \" \".join(new_text)\n",
        "\n",
        "        # Calling `chat_words_conversion` for each tweet\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(\n",
        "            lambda text: chat_words_conversion(str(text))\n",
        "        )\n",
        "\n",
        "    def final_parenthesis(self):\n",
        "        \"\"\"\n",
        "        Substitutes the final parenthesis of a tweet with a positive or negative smile.\n",
        "        More on this in the report.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Substituting final paranthesis...\")\n",
        "\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\)+$\", \":)\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\(+$\", \":(\")\n",
        "\n",
        "    def final_parenthesis_to_tags(self):\n",
        "        \"\"\"\n",
        "        Substitutes the final parenthesis of a tweet with a positive or negative smile tag.\n",
        "        More on this in the report.\n",
        "        \"\"\"\n",
        "        print(\"Substituting final paranthesis with tags...\")\n",
        "\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\)+$\", \" <smile> \")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\(+$\", \" <sadface> \")\n",
        "\n",
        "    def remove_numbers(self):\n",
        "        \"\"\"\n",
        "        Removes numbers from each tweet\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Removing numbers...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\d\", \"\")\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        \"\"\"\n",
        "        Removes everything that is not alphanumeric and not a space.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Removing punctuation...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"[^\\w\\s]\", \"\")\n",
        "\n",
        "    def to_lower(self):\n",
        "        \"\"\"\n",
        "        Converts each tweet to lowercase.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Converting to lowercase...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.lower()\n",
        "\n",
        "    def correct_spelling(self):\n",
        "        \"\"\"\n",
        "        Corrects spelling of each tweet.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Correcting spelling...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(\n",
        "            lambda text: Preprocessing.__correct_spelling(text)\n",
        "        )\n",
        "\n",
        "    def lemmatize(self):\n",
        "        \"\"\"\n",
        "        Performs the lemmatization.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Performing lemmatization...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(Preprocessing.__lemmatize)\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        \"\"\"\n",
        "        Removes english stopwords.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Removing stopwords...\")\n",
        "\n",
        "        # Getting english stopwords set\n",
        "        stopwords_ = set(stopwords.words(\"english\"))\n",
        "\n",
        "        # Removing stopwords for each tweet\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(\n",
        "            lambda text: \" \".join(\n",
        "                [word for word in str(text).split() if word not in stopwords_]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def empty_tweets(self):\n",
        "        \"\"\"\n",
        "        Adds tag <EMPTY> for empty tweets.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Marking empty tweets...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"^\\s*$\", \"<EMPTY>\")\n",
        "\n",
        "    def remove_elongs(self):\n",
        "        \"\"\"\n",
        "        Removes elongs. (e.g.: hellooooo -> hello)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Removing elongs...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(\n",
        "            lambda text: str(re.sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2\", text))\n",
        "        )\n",
        "\n",
        "    def correct_spacing_indexing(self):\n",
        "        \"\"\"\n",
        "        Deletes double or more spaces and corrects indexing.\n",
        "\n",
        "        Must be called after calling the above methods.\n",
        "        Most of the above methods just delete a token. However since tokens are\n",
        "        surrounded by whitespaces, they will often result in having more than one\n",
        "        space between words.\n",
        "\n",
        "        The only exception is for `remove_space_between_emoticons` method.\n",
        "        Should be called before and after calling that method.\n",
        "        It could exist ':  )' which that method doesn't recognize.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Correcting spacing...\")\n",
        "\n",
        "        # Removing double spaces\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\s{2,}\", \" \")\n",
        "\n",
        "        # Stripping text\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(lambda text: text.strip())\n",
        "\n",
        "        # Correcting the indexing\n",
        "        self.__data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    def remove_space_between_emoticons(self):\n",
        "        \"\"\"\n",
        "        Removes spaces between emoticons (e.g.: ': )' --> ':)').\n",
        "        Adds a space between a word and an emoticon (e.g.: 'hello:)' --> 'hello :)')\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Removing space between emoticons...\")\n",
        "\n",
        "        # Getting list of all emoticons\n",
        "        emo_list = [el for value in list(EMOTICONS_GLOVE.values()) for el in value]\n",
        "\n",
        "        # Putting a space between each character in each emoticon\n",
        "        emo_with_spaces = \"|\".join(re.escape(\" \".join(emo)) for emo in emo_list)\n",
        "\n",
        "        # Getting all emoticons that don't contain any alphanumeric character\n",
        "        all_non_alpha_emo = \"|\".join(\n",
        "            re.escape(emo)\n",
        "            for emo in emo_list\n",
        "            if not any(char.isalpha() or char.isdigit() for char in emo)\n",
        "        )\n",
        "\n",
        "        # Removing spaces between emoticons\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            emo_with_spaces, lambda t: t.group().replace(\" \", \"\")\n",
        "        )\n",
        "\n",
        "        # Adding space between a word and an emoticon\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            rf\"({all_non_alpha_emo})\", r\" \\1 \"\n",
        "        )\n",
        "\n",
        "    def emoticons_to_tags(self):\n",
        "        \"\"\"\n",
        "        Convert emoticons (with or without spaces) into tags\n",
        "          according to the pretrained stanford glove model\n",
        "          (e.g.: :) ---> <smile> and so on)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Converting emoticons to tags...\")\n",
        "\n",
        "        # Dictionary like {tag:[list_of_emoticons]}\n",
        "        union_re = {}\n",
        "        for tag, emo_list in EMOTICONS_GLOVE.items():\n",
        "            # Getting emoticons as they are\n",
        "            re_emo = \"|\".join(re.escape(emo) for emo in emo_list)\n",
        "            union_re[tag] = re_emo\n",
        "\n",
        "        # Function to be called for each tweet\n",
        "        def inner(text, _union_re):\n",
        "            for tag, union_re in _union_re.items():\n",
        "                text = re.sub(union_re, \" \" + tag + \" \", text)\n",
        "            return text\n",
        "\n",
        "        # Applying for each tweet\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].apply(\n",
        "            lambda text: inner(str(text), union_re)\n",
        "        )\n",
        "\n",
        "    def hashtags_to_tags(self):\n",
        "        \"\"\"\n",
        "        Convert hashtags. (e.g.: #hello ---> <hashtag> hello)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Converting hashtags to tags...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            r\"#(\\S+)\", r\"<hashtag> \\1\"\n",
        "        )\n",
        "\n",
        "    def numbers_to_tags(self):\n",
        "        \"\"\"\n",
        "        Convert numbers into tags. (e.g.: 34 ---> <number>)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Converting numbers to tags...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", r\"<number>\"\n",
        "        )\n",
        "\n",
        "    def repeat_to_tags(self):\n",
        "        \"\"\"\n",
        "        Convert repetitions of '!' or '?' or '.' into tags.\n",
        "          (e.g.: ... ---> . <repeat>)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Converting repetitions of symbols to tags...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            r\"([!?.]){2,}\", r\"\\1 <repeat>\"\n",
        "        )\n",
        "\n",
        "    def elongs_to_tags(self):\n",
        "        \"\"\"\n",
        "        Convert elongs into tags. (e.g.: hellooooo ---> hello <elong>)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Converting elongated words to tags...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
        "            r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\"\n",
        "        )\n",
        "\n",
        "    def remove_endings(self):\n",
        "        \"\"\"\n",
        "        Remove ... <url> which represents the ending of tweet\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Removing tweet ending when the tweet is cropped...\")\n",
        "        self.__data[\"text\"] = self.__data[\"text\"].str.replace(r\"\\.{3} <url>$\", \"\")\n",
        "\n",
        "    # STATIC METHODS (private, used internally)\n",
        "\n",
        "    # Instance of `SymSpell` class\n",
        "    symspell = None\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_symspell():\n",
        "        \"\"\"\n",
        "        Instantiates a `SymSpell` object.\n",
        "\n",
        "        :return: instantiated object\n",
        "        :rtype: SymSpell\n",
        "        \"\"\"\n",
        "\n",
        "        # If is not already instantiated\n",
        "        if Preprocessing.symspell is None:\n",
        "            # Instantiating `SymSpell`\n",
        "            Preprocessing.symspell = SymSpell()\n",
        "\n",
        "            # Getting dictionary for single words\n",
        "            dictionary_path = pkg_resources.resource_filename(\n",
        "                \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
        "            )\n",
        "            Preprocessing.symspell.load_dictionary(\n",
        "                dictionary_path, term_index=0, count_index=1\n",
        "            )\n",
        "\n",
        "            # Getting dictionary for bigram (two words)\n",
        "            bigram_path = pkg_resources.resource_filename(\n",
        "                \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
        "            )\n",
        "            Preprocessing.symspell.load_bigram_dictionary(\n",
        "                bigram_path, term_index=0, count_index=2\n",
        "            )\n",
        "\n",
        "        return Preprocessing.symspell\n",
        "\n",
        "    @staticmethod\n",
        "    def __word_segmentation(text):\n",
        "        \"\"\"\n",
        "        Tries to put spaces between word in a text (used for hashtag).\n",
        "          (e.g.: helloguys --> hello guys))\n",
        "\n",
        "        :param text: Text to be converted (typically an hashtag)\n",
        "        :type text: str\n",
        "        :return: Processed text\n",
        "        :rtype: str\n",
        "        \"\"\"\n",
        "\n",
        "        # `max_edit_distance = 0` avoids that `SymSpell` corrects spelling.\n",
        "        result = Preprocessing.__get_symspell().word_segmentation(\n",
        "            text, max_edit_distance=0\n",
        "        )\n",
        "        return result.segmented_string\n",
        "\n",
        "    @staticmethod\n",
        "    def __correct_spelling(text):\n",
        "        \"\"\"\n",
        "        Corrects spelling of a word (e.g.: helo -> hello)\n",
        "\n",
        "        :param text: Text to be converted\n",
        "        :type text: str\n",
        "        :return: Processed text\n",
        "        :rtype: str\n",
        "        \"\"\"\n",
        "\n",
        "        # `max_edit_distance = 2` tells `SymSpell` to check at a maximum distance\n",
        "        #  of 2 in the vocabulary. Only words with at most 2 letters wrong will be corrected.\n",
        "        result = Preprocessing.__get_symspell().lookup_compound(\n",
        "            text, max_edit_distance=2\n",
        "        )\n",
        "\n",
        "        return result[0].term\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_wordnet_tag(nltk_tag):\n",
        "        \"\"\"\n",
        "        Returns type of word according to nltk pos tag.\n",
        "\n",
        "        :param nltk_tag: nltk pos tag\n",
        "        :type nltk_tag: list(tuple(str, str))\n",
        "        :return: type of a word\n",
        "        :rtype: str\n",
        "        \"\"\"\n",
        "\n",
        "        if nltk_tag.startswith(\"V\"):\n",
        "            return wordnet.VERB\n",
        "        elif nltk_tag.startswith(\"N\"):\n",
        "            return wordnet.NOUN\n",
        "        elif nltk_tag.startswith(\"J\"):\n",
        "            return wordnet.ADJ\n",
        "        elif nltk_tag.startswith(\"R\"):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            # This is the default in WordNetLemmatizer, when no pos tag is passed\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    @staticmethod\n",
        "    def __lemmatize(text):\n",
        "        \"\"\"\n",
        "        Performs lemmatization using nltk pos tag and `WordNetLemmatizer`.\n",
        "\n",
        "        :param text: Text to be processed\n",
        "        :type text: str\n",
        "        :return: processed texg\n",
        "        :rtype: str\n",
        "        \"\"\"\n",
        "\n",
        "        nltk_tagged = nltk.pos_tag(text.split())\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        return \" \".join(\n",
        "            [\n",
        "                lemmatizer.lemmatize(w, Preprocessing.__get_wordnet_tag(nltk_tag))\n",
        "                for w, nltk_tag in nltk_tagged\n",
        "            ]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "oYPdtAmkazbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract method"
      ],
      "metadata": {
        "id": "pBi70MbkDDIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7wKWK7K3lBK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractModel(ABC):\n",
        "    def __init__(self, weights_path: str):\n",
        "        self.__weights_path = weights_path\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_preprocessing_methods(self, is_test: bool = False):\n",
        "        pass\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit_predict(self, X, y, ids_test, X_test, prediction_path):\n",
        "        pass\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, ids, X, path):\n",
        "        pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_submission(ids: list[int], predictions: list[int], path: str):\n",
        "        # Generating the submission file\n",
        "        submission = pd.DataFrame(columns=[\"Id\", \"Prediction\"],\n",
        "                                data={\"Id\": ids, \"Prediction\": predictions})\n",
        "\n",
        "        # For many models the labels are 0 or 1. Replacing 0s with -1s.\n",
        "        submission[\"Prediction\"].replace(0, -1, inplace=True)\n",
        "\n",
        "        # Saving the file\n",
        "        submission.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_data(X: pd.DataFrame, y: pd.DataFrame, test_size: float = 0.2, random_state: int = 42, **kwargs) -> tuple:\n",
        "        print(\"Splitting data in train and test set...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, **kwargs)\n",
        "\n",
        "        return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "MZ81hz7cDHsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "aXatFg_9EHr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHBmVMce13HX",
        "outputId": "6d492839-cff6-4ef8-9e11-db56579f6535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "rdo14Zw4EBHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gru(AbstractModel):\n",
        "    def __init__(self,\n",
        "                 weights_path: str = WEIGHT_PATH,\n",
        "                 glove_path: str = GLOVE_PATH,\n",
        "                 max_tweet_length: int = 120,\n",
        "                 embedding_dim: int = 100):\n",
        "        super().__init__(weights_path)\n",
        "\n",
        "        self.__tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
        "        self.__model = tf.keras.Sequential()\n",
        "        self.__max_tweet_length = max_tweet_length\n",
        "        self.__embedding_dim = embedding_dim\n",
        "        self.__glove_path = glove_path\n",
        "\n",
        "        # Size of the vocabulary, it will be updated according to the input data\n",
        "        self.__vocab_size = 0\n",
        "\n",
        "\n",
        "    def update_vocabulary(self, X: pd.DataFrame):\n",
        "\n",
        "        # Updates the default internal vocabulary according to the words in X\n",
        "        self.__tokenizer.fit_on_texts(X)\n",
        "\n",
        "        # Updating the vocabulary length.\n",
        "        # NOTE: the +2 is due to some special reserved tokens that are in the vocabulary\n",
        "        # but not in the tweets\n",
        "        self.__vocab_size = len(self.__tokenizer.word_index) + 2\n",
        "\n",
        "\n",
        "    def __convert_data(self, X: pd.DataFrame):\n",
        "\n",
        "        # Creating the numerical tokens and padding each tweet to max_tweet_length\n",
        "        X_tokens = self.__tokenizer.texts_to_sequences(X)\n",
        "\n",
        "        # NOTE: padding = \"post\" means that the pad is after each sequence\n",
        "        # (each tweet) and not before\n",
        "        X_pad = pad_sequences(\n",
        "            X_tokens,\n",
        "            maxlen=self.__max_tweet_length,\n",
        "            padding=\"post\"\n",
        "        )\n",
        "\n",
        "        return X_pad\n",
        "\n",
        "\n",
        "    def __generate_embedding_matrix(self):\n",
        "\n",
        "        # Getting the vocabulary from the tokenizer\n",
        "        word_index = self.__tokenizer.word_index\n",
        "\n",
        "        # Creating a dictionary the embedding file. Keys = words in the embedding file,\n",
        "        # Values = their respective vector\n",
        "        embeddings_index = {}\n",
        "\n",
        "        with open(self.__glove_path) as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                embeddings_index[word] = coefs\n",
        "\n",
        "        # Printing the number of words found in the file\n",
        "        print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "        # Generating the embedding matrix\n",
        "        embedding_matrix = np.zeros((self.__vocab_size, self.__embedding_dim))\n",
        "\n",
        "        # These two variables will hold the number of words in the vocabulary\n",
        "        # That are found in the file, and the number of the ones that are not.\n",
        "        hits = 0\n",
        "        misses = 0\n",
        "\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "        # Words not found in embedding index will be represented as a zero-vector.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            misses += 1\n",
        "\n",
        "        # Printing the number of found / not found words\n",
        "        print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "        return embedding_matrix\n",
        "\n",
        "\n",
        "    def __build_model(self, embedding_matrix):\n",
        "        # Creating the model with all its layers.\n",
        "        # NOTE: mask_zero must be true because 0 is a special character\n",
        "        # used as padding, as mentioned before.\n",
        "        # The Embedding layer is not trainable since we loaded the vectors from a pre-trained file,\n",
        "        # as mentioned before\n",
        "        self.__model.add(layers.Embedding(\n",
        "            input_dim=self.__vocab_size,\n",
        "            output_dim=self.__embedding_dim,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            input_length=self.__max_tweet_length,\n",
        "            mask_zero=True,\n",
        "            trainable=False\n",
        "        ))\n",
        "\n",
        "        # NOTE: since we are using GRU as a RNN, we need to define two types of dropouts: the\n",
        "        # first one is used for the first operation on the inputs (when data\n",
        "        # \"enters\" in GRU) the second one is used for the recurrences Units\n",
        "        self.__model.add(layers.Bidirectional(\n",
        "        layers.GRU(units=100, dropout=0.2, recurrent_dropout=0, activation=\"tanh\",\n",
        "                   recurrent_activation=\"sigmoid\", unroll=False, use_bias=True,\n",
        "                   reset_after=True)))\n",
        "        self.__model.add(tf.keras.layers.Dense(100, activation=\"relu\")),\n",
        "        self.__model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        # Compiling the model. The optimizer is Adam with standard lr (0.001)\n",
        "        self.__model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "        # Printing model\"s summary\n",
        "        print(self.__model.summary())\n",
        "\n",
        "\n",
        "    def get_preprocessing_methods(self, is_test: bool = False):\n",
        "        methods = []\n",
        "\n",
        "        if not is_test:\n",
        "        # Dropping duplicates tweets only in the training set\n",
        "            methods.append(\"drop_duplicates\")\n",
        "\n",
        "        methods.extend([\n",
        "            \"remove_tags\",\n",
        "            \"remove_whitespace\",\n",
        "            \"remove_space_between_emoticons\",\n",
        "            \"remove_whitespace\",\n",
        "            \"emoticons_to_tags\",\n",
        "            \"final_emoticons_to_tag\",\n",
        "            \"numbers_to_tags\",\n",
        "            \"hashtags_to_tags\",\n",
        "            \"repeat_to_tags\",\n",
        "            \"elongs_to_tags\",\n",
        "            \"lower\",\n",
        "            \"remove_whitespace\"\n",
        "        ])\n",
        "\n",
        "        return methods\n",
        "\n",
        "\n",
        "    def fit_predict(self,\n",
        "                    X: pd.DataFrame,\n",
        "                    y: pd.DataFrame,\n",
        "                    ids_test: np.ndarray,\n",
        "                    X_test: np.ndarray,\n",
        "                    prediction_path: str,\n",
        "                    batch_size: int = 128,\n",
        "                    epochs: int = 10):\n",
        "        # Splitting train and validation data\n",
        "        X_train, X_val, y_train, y_val = AbstractModel._split_data(X, y)\n",
        "\n",
        "        # Converting train and validation data to sequences (vectors)\n",
        "        X_train_pad = self.__convert_data(X_train)\n",
        "        X_val_pad = self.__convert_data(X_val)\n",
        "\n",
        "        # Generating the embedding matrix from the training data\n",
        "        embedding_matrix = self.__generate_embedding_matrix()\n",
        "\n",
        "        # Building the model\n",
        "        self.__build_model(embedding_matrix)\n",
        "\n",
        "        print(\"Training the model...\")\n",
        "        self.__model.fit(X_train_pad, y_train, batch_size, epochs,\n",
        "                        validation_data=(X_val_pad, y_val))\n",
        "\n",
        "        print(\"Saving the model...\")\n",
        "        self.__model.save(f\"{self._weights_path}model\")\n",
        "\n",
        "        print(\"Making the prediction...\")\n",
        "        self.predict(ids_test, X_test, prediction_path, from_weights=False)\n",
        "\n",
        "\n",
        "    def predict(self, ids, X, path, from_weights=True):\n",
        "        if from_weights:\n",
        "            # Loading weights\n",
        "            self.__model = tf.keras.models.load_model(f\"{self._weights_path}model\")\n",
        "\n",
        "        # Converting input data\n",
        "        X_pad = self.__convert_data(X)\n",
        "        predictions = self.__model.predict(X_pad).squeeze()\n",
        "        preds = np.where(predictions >= 0.5, 1, -1)\n",
        "        print(preds)\n",
        "\n",
        "        # Creating and saving the file\n",
        "        AbstractModel._create_submission(ids, preds, path)"
      ],
      "metadata": {
        "id": "Bv1bzZisEQTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGRU(AbstractModel):\n",
        "    \"\"\"\n",
        "    This class implements a Gru bidirectional neural network with Glove pretrained embedding file.\n",
        "    The embedding file has been created by Stanford University, and it's based on tweets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights_path=WEIGHT_PATH, glove_path=GLOVE_PATH, max_tweet_length=120,\n",
        "                embedding_dim=100):\n",
        "        \"\"\"\n",
        "        :param weights_path: weights path of the model. Model's parameters will be loaded and saved from this path.\n",
        "        :type weights_path: str\n",
        "        :param glove_path: path of the glove file.\n",
        "        :type glove_path: str\n",
        "        :param max_tweet_length: maximum (estimated) lenght of a tweet in words.\n",
        "        We exaggerated the dimension to be sure to not truncate any tweet.\n",
        "        :type max_tweet_length: int, optional\n",
        "        :param embedding_dim: the embedding dimension. Every word is represented by a vector of this length\n",
        "        in the embedding space. Please before changing it refer to your embedding file documentation.\n",
        "        :type embedding_dim: int, optional\n",
        "        \"\"\"\n",
        "        super().__init__(weights_path)\n",
        "\n",
        "        self.__tokenizer = Tokenizer(oov_token='<unk>')\n",
        "        self.__model = tf.keras.Sequential()\n",
        "        self.__max_tweet_length = max_tweet_length\n",
        "        self.__embedding_dim = embedding_dim\n",
        "        self.__glove_path = glove_path\n",
        "\n",
        "        # Size of the vocabulary, it will be updated according to the input data\n",
        "        self.__vocab_size = 0\n",
        "\n",
        "    def update_vocabulary(self, X):\n",
        "        \"\"\"\n",
        "        Method used to update (create) the vocabulary of the tokenizer.\n",
        "\n",
        "        :param X: A matrix. Each row is a document, in our case a tweet.\n",
        "        :type X: numpy.ndarray\n",
        "        \"\"\"\n",
        "\n",
        "        print('Updating vocabulary...')\n",
        "\n",
        "        # Updates the default internal vocabulary according to the words in X\n",
        "        self.__tokenizer.fit_on_texts(X)\n",
        "\n",
        "        # Updating the vocabulary length.\n",
        "        # NOTE: the +2 is due to some special reserved tokens that are in the vocabulary\n",
        "        # but not in the tweets\n",
        "        self.__vocab_size = len(self.__tokenizer.word_index) + 2\n",
        "\n",
        "    def __convert_data(self, X):\n",
        "        \"\"\"\n",
        "        Converts the tweets in numerical tokens.\n",
        "        Each word in the tweet is substituted with its index in the vocabulary,\n",
        "        in a bag of words fashion. Each tweet is padded to 120 words at maximum,\n",
        "        with 0 as special padding character.\n",
        "\n",
        "        param X: A matrix. Each row is a document, in our case a tweet.\n",
        "        :type X: numpy.ndarray\n",
        "\n",
        "        :return: Numpy array with shape (len(X), max_tweet_length)\n",
        "        :rtype: numpy.ndarray\n",
        "        \"\"\"\n",
        "\n",
        "        print('Converting data...')\n",
        "        print(X)\n",
        "\n",
        "        # Creating the numerical tokens and padding each tweet to max_tweet_length\n",
        "        X_tokens = self.__tokenizer.texts_to_sequences(X)\n",
        "        print(\"X_tokens\")\n",
        "        print(X_tokens[0])\n",
        "\n",
        "        # NOTE: padding = 'post' means that the pad is after each sequence\n",
        "        # (each tweet) and not before\n",
        "        X_pad = pad_sequences(\n",
        "            X_tokens,\n",
        "            maxlen=self.__max_tweet_length,\n",
        "            padding='post'\n",
        "        )\n",
        "\n",
        "        return X_pad\n",
        "\n",
        "    def __generate_embedding_matrix(self):\n",
        "        \"\"\"\n",
        "        Generates the word embedding matrix according to the words in the vocabulary.\n",
        "        Each word is represented by a vector with length equal to embedding_dim.\n",
        "        The embedding is done according to a model pretrained on twitter data\n",
        "        (https://nlp.stanford.edu/projects/glove/). Only the words in the vocabulary that\n",
        "        are found in the pretrained model are taken into account.\n",
        "\n",
        "        :return: The embedding matrix. Each row corresponds to a word in the vocabulary.\n",
        "        The index of the row is the index of the word in the voc.\n",
        "        :rtype: numpy.ndarray\n",
        "        \"\"\"\n",
        "\n",
        "        print('Generating embedding matrix...')\n",
        "\n",
        "        # Getting the vocabulary from the tokenizer\n",
        "        word_index = self.__tokenizer.word_index\n",
        "\n",
        "        # Creating a dictionary the embedding file. Keys = words in the embedding file,\n",
        "        # Values = their respective vector\n",
        "        embeddings_index = {}\n",
        "\n",
        "        with open(self.__glove_path) as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                embeddings_index[word] = coefs\n",
        "\n",
        "        # Printing the number of words found in the file\n",
        "        print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "        # Generating the embedding matrix\n",
        "        embedding_matrix = np.zeros((self.__vocab_size, self.__embedding_dim))\n",
        "\n",
        "        # These two variables will hold the number of words in the vocabulary\n",
        "        # That are found in the file, and the number of the ones that are not.\n",
        "        hits = 0\n",
        "        misses = 0\n",
        "\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "        # Words not found in embedding index will be represented as a zero-vector.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            misses += 1\n",
        "\n",
        "        # Printing the number of found / not found words\n",
        "        print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "        return embedding_matrix\n",
        "\n",
        "    def __build_model(self, embedding_matrix):\n",
        "        \"\"\"\n",
        "        Method used to build and compile the GRU (Bidirectional) model.\n",
        "\n",
        "        :param embedding_matrix: The embedding matrix used for the Embedding layer of the model.\n",
        "        The embedding happens according to the matrix. The matrix is built in the previous method.\n",
        "        :type: numpy.ndarray:\n",
        "        \"\"\"\n",
        "\n",
        "        print('Building model...')\n",
        "\n",
        "        # Creating the model with all its layers.\n",
        "        # NOTE: mask_zero must be true because 0 is a special character\n",
        "        # used as padding, as mentioned before.\n",
        "        # The Embedding layer is not trainable since we loaded the vectors from a pre-trained file,\n",
        "        # as mentioned before\n",
        "        self.__model.add(layers.Embedding(\n",
        "        input_dim=self.__vocab_size,\n",
        "        output_dim=self.__embedding_dim,\n",
        "        embeddings_initializer=Constant(embedding_matrix),\n",
        "        input_length=self.__max_tweet_length,\n",
        "        mask_zero=True,\n",
        "        trainable=False))\n",
        "\n",
        "        # NOTE: since we are using GRU as a RNN, we need to define two types of dropouts: the\n",
        "        # first one is used for the first operation on the inputs (when data\n",
        "        # \"enters\" in GRU) the second one is used for the recurrences Units\n",
        "        self.__model.add(layers.Bidirectional(\n",
        "        layers.GRU(units=100, dropout=0.2, recurrent_dropout=0, activation='tanh', \\\n",
        "                    recurrent_activation='sigmoid', unroll=False, use_bias=True,\n",
        "                    reset_after=True)))\n",
        "        self.__model.add(tf.keras.layers.Dense(100, activation='relu')),\n",
        "        self.__model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # Compiling the model. The optimizer is Adam with standard lr (0.001)\n",
        "        self.__model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "        # Printing model's summary\n",
        "        print(self.__model.summary())\n",
        "\n",
        "    def get_preprocessing_methods(self, istest=False):\n",
        "        methods = []\n",
        "\n",
        "        if not istest:\n",
        "        # Dropping duplicates tweets only in the training set\n",
        "            methods.append('drop_duplicates')\n",
        "\n",
        "        methods.extend([\n",
        "        'remove_endings',\n",
        "        'correct_spacing_indexing',\n",
        "        'remove_space_between_emoticons',\n",
        "        'correct_spacing_indexing',\n",
        "        'emoticons_to_tags',\n",
        "        'final_parenthesis_to_tags',\n",
        "        'numbers_to_tags',\n",
        "        'hashtags_to_tags',\n",
        "        'repeat_to_tags',\n",
        "        'elongs_to_tags',\n",
        "        'to_lower',\n",
        "        'correct_spacing_indexing'\n",
        "        ])\n",
        "\n",
        "        return methods\n",
        "\n",
        "    def fit_predict(self, X, Y, ids_test, X_test, prediction_path, batch_size=128,\n",
        "                    epochs=10):\n",
        "        \"\"\"\n",
        "        Fits (train) the model, and makes a prediction on the test data.\n",
        "\n",
        "        :param X: datapoint matrix. Will be splitted into training and validation data.\n",
        "        :type X: numpy.ndarray\n",
        "        :param Y: labels of the datapoints.\n",
        "        :type Y: numpy.ndarray\n",
        "        :param ids_test: the ids of the test datapoints, necessary to make a prediction.\n",
        "        :type ids_test: numpy.ndarray\n",
        "        :param X_test: the matrix containing the test datapoints for the prediction.\n",
        "        :type X_test: numpy.ndarray\n",
        "        :param prediction_path: relative path of the prediction file.\n",
        "        :type prediction_path: str\n",
        "        :param batch_size: size of the mini-batches used when training the model.\n",
        "        :type batch_size: int, optional\n",
        "        :param epochs: number of epochs used when training the model.\n",
        "        :type epochs: int, optional\n",
        "        \"\"\"\n",
        "\n",
        "        # Splitting train and validation data\n",
        "        X_train, X_val, Y_train, Y_val = AbstractModel._split_data(X, Y)\n",
        "\n",
        "        # Converting train and validation data to sequences (vectors)\n",
        "        X_train_pad = self.__convert_data(X_train)\n",
        "        X_val_pad = self.__convert_data(X_val)\n",
        "\n",
        "        # Generating the embedding matrix from the training data\n",
        "        embedding_matrix = self.__generate_embedding_matrix()\n",
        "\n",
        "        # Building the model\n",
        "        self.__build_model(embedding_matrix)\n",
        "\n",
        "        print('Training the model...')\n",
        "        self.__model.fit(X_train_pad, Y_train, batch_size, epochs,\n",
        "                        validation_data=(X_val_pad, Y_val))\n",
        "\n",
        "        print('Saving the model...')\n",
        "        self.__model.save(f'{self._weights_path}model')\n",
        "\n",
        "        print('Making the prediction...')\n",
        "        self.predict(ids_test, X_test, prediction_path, from_weights=False)\n",
        "\n",
        "    def predict(self, ids, X, path, from_weights=True):\n",
        "        \"\"\"\n",
        "        Performs the predictions. Usually called within the fit_predict method.\n",
        "\n",
        "        :param ids: ids of testing data.\n",
        "        :type ids: numpy.ndarray\n",
        "        :param X: matrix of the testing datapoints.\n",
        "        :type x: numpy.ndarray\n",
        "        :param path: specifies where to store the submission file\n",
        "        :type path: str\n",
        "        :param from_weights: specifies if it is a prediction of a new model or if it is made according to a pre-trained one.\n",
        "        :type from_weights: bool, optional\n",
        "        \"\"\"\n",
        "\n",
        "        if from_weights:\n",
        "        # Loading weights\n",
        "            self.__model = tf.keras.models.load_model(f'{self._weights_path}model')\n",
        "\n",
        "        # Converting input data\n",
        "        X_pad = self.__convert_data(X)\n",
        "        predictions = self.__model.predict(X_pad).squeeze()\n",
        "        preds = np.where(predictions >= 0.5, 1, -1)\n",
        "        print(preds)\n",
        "\n",
        "        # Creating and saving the file\n",
        "        AbstractModel._create_submission(ids, preds, path)"
      ],
      "metadata": {
        "id": "DpBtWdyuLs66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "NAAJYjgdQ2ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import strftime"
      ],
      "metadata": {
        "id": "zT0DU6NzO_Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@function_name\n",
        "def run_preprocessing(cls: AbstractModel, train_path: str, test_path: str, is_full: bool = False):\n",
        "    path_ls = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH] if is_full else [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
        "\n",
        "    train_preprocessing = Preprocessing(path_ls)\n",
        "    test_preprocessing = Preprocessing([TEST_PATH], is_submission=True)\n",
        "\n",
        "    for method in cls().get_preprocessing_methods(is_test=False):\n",
        "        getattr(train_preprocessing, method)()\n",
        "\n",
        "    for method in cls().get_preprocessing_methods(is_test=True):\n",
        "        getattr(test_preprocessing, method)()\n",
        "\n",
        "    # Save it\n",
        "    train_df = train_preprocessing.get()\n",
        "    train_df = train_df.sample(frac=1)\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    test_preprocessing.get().to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "AmayaJUcrYsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_new_preprocessing(\n",
        "    csr: AbstractModel, train_preprocessed_path=TRAIN_PREP_PATH, test_preprocessed_path=TEST_PREP_PATH, full_data=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the preprocessing methods according to the chosen classifier\n",
        "      on the train and test data\n",
        "\n",
        "    :param csr: chosen classifier (child of AbstractModel)\n",
        "    :type csr: AbstractModel\n",
        "    :param train_preprocessed_path: path to load train data\n",
        "    :type train_preprocessed_path: str\n",
        "    :param test_preprocessed_path: path to load test data\n",
        "    :type test_preprocessed_path: str\n",
        "    :param full_data: if False, the small dataset (200K rows) is used\n",
        "    :type full_data: bool, optional\n",
        "    \"\"\"\n",
        "\n",
        "    # Read data\n",
        "    if full_data:\n",
        "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
        "    else:\n",
        "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
        "\n",
        "    train_preprocessing = NewPreprocessing(dataset_files, submission=False)\n",
        "    test_preprocessing = NewPreprocessing([TEST_PATH], submission=True)\n",
        "\n",
        "    # Preprocess it\n",
        "    for method in csr.get_preprocessing_methods(istest=False):\n",
        "        getattr(train_preprocessing, method)()\n",
        "\n",
        "    for method in csr.get_preprocessing_methods(istest=True):\n",
        "        getattr(test_preprocessing, method)()\n",
        "\n",
        "    # Save it\n",
        "    train_df = train_preprocessing.get()\n",
        "    train_df = train_df.sample(frac=1)\n",
        "\n",
        "    train_df.to_csv(train_preprocessed_path, index=False)\n",
        "    test_preprocessing.get().to_csv(test_preprocessed_path, index=False)\n"
      ],
      "metadata": {
        "id": "GPgmjORgbEEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class Models(Enum):\n",
        "    gru = 'gru'\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Returns the value of the Enumeration\n",
        "\n",
        "        :return: value of Enumeration\n",
        "        :rtype: str\n",
        "        \"\"\"\n",
        "        return self.value\n",
        "\n",
        "    def get_model_name(self):\n",
        "        \"\"\"\n",
        "        Performs a mapping between Models value and class/string to run the method\n",
        "\n",
        "        :return: class/string with respect to the value of the Enumeration\n",
        "        :rtype: object\n",
        "        \"\"\"\n",
        "\n",
        "        list_model = {\n",
        "            Models.gru: NewGRU,\n",
        "        }\n",
        "\n",
        "        return list_model[self]"
      ],
      "metadata": {
        "id": "LZ3mSmJVPr8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute(\n",
        "    weights_path: str = WEIGHT_PATH,\n",
        "    is_prep: bool = False,\n",
        "    train_preprocessed_path: str = TRAIN_PREP_PATH,\n",
        "    test_preprocessed_path: str = TEST_PREP_PATH,\n",
        "    submission_path: str = f\"{GLOBAL_PATH}/submission\",\n",
        "    full_data: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a submission file using a method specified by user.\n",
        "      If specified, loads preprocessed data and/or the weights, otherwise\n",
        "      preprocesses data, fits the model and makes predictions from scratch\n",
        "\n",
        "    :param args: arguments chosen by the user\n",
        "    :type args: argparse.Namespace\n",
        "    :param weights_path: path to load/store the weights\n",
        "    :type weights_path: str\n",
        "    :param train_preprocessed_path: path to load/store the train preprocessed data\n",
        "    :type train_preprocessed_path: str\n",
        "    :param test_preprocessed_path: path to load/store the test preprocessed data\n",
        "    :type test_preprocessed_path: str\n",
        "    :param submission_path: path to save the submission file\n",
        "    :type submission_path: str\n",
        "    :param full_data: if False, the small dataset (200K rows) is used\n",
        "    :type full_data: bool, optional\n",
        "    :param kwargs: additional arguments for classical methods (otherwise empty)\n",
        "    :type kwargs: dict\n",
        "    \"\"\"\n",
        "\n",
        "    classifier = Models[\"gru\"].get_model_name()(weights_path)\n",
        "\n",
        "    # Specifying the columns of the DataFrame\n",
        "    usecols_train = [\"text\", \"label\"]\n",
        "    usecols_test = [\"ids\", \"text\"]\n",
        "\n",
        "    if not is_prep:\n",
        "        run_new_preprocessing(classifier, train_preprocessed_path, test_preprocessed_path)\n",
        "\n",
        "    # Loading preprocessed data\n",
        "    train_preprocessed = pd.read_csv(train_preprocessed_path, usecols=usecols_train)\n",
        "    test_preprocessed = pd.read_csv(test_preprocessed_path, usecols=usecols_test)\n",
        "\n",
        "    # Dropping null rows from training data\n",
        "    train_preprocessed.dropna(inplace=True)\n",
        "\n",
        "    X, Y = train_preprocessed[\"text\"].values, train_preprocessed[\"label\"].values\n",
        "    X_test, test_ids = (\n",
        "        test_preprocessed[\"text\"].values,\n",
        "        test_preprocessed[\"ids\"].values,\n",
        "    )\n",
        "\n",
        "    # Updating the vocabulary of the GRU classifier according to the training data\n",
        "    classifier.update_vocabulary(X)\n",
        "\n",
        "    classifier.fit_predict(\n",
        "        X,\n",
        "        Y,\n",
        "        test_ids,\n",
        "        X_test,\n",
        "        f'{submission_path}submission-{strftime(\"%Y-%m-%d_%H:%M:%S\")}.csv'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "hqkLongSQFhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute()"
      ],
      "metadata": {
        "id": "XDqD7AvrDxv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddb7db6d-62b9-460c-c8e6-474e48494d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  self.__data = self.__data.append(df).reset_index(drop=True)\n",
            "<ipython-input-24-f2f0be839fb7>:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  self.__data = self.__data.append(df).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping duplicates...\n",
            "Removing tweet ending when the tweet is cropped...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:400: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(r\"\\.{3} <url>$\", \"\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcting spacing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:287: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\s{2,}\", \" \")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing space between emoticons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:317: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n",
            "<ipython-input-24-f2f0be839fb7>:322: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcting spacing...\n",
            "Converting emoticons to tags...\n",
            "Substituting final paranthesis with tags...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:190: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\)+$\", \" <smile> \")\n",
            "<ipython-input-24-f2f0be839fb7>:191: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\"\\(+$\", \" <sadface> \")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting numbers to tags...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:369: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting hashtags to tags...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:359: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting repetitions of symbols to tags...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:380: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting elongated words to tags...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-f2f0be839fb7>:390: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.__data[\"text\"] = self.__data[\"text\"].str.replace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting to lowercase...\n",
            "Correcting spacing...\n",
            "Removing tweet ending when the tweet is cropped...\n",
            "Correcting spacing...\n",
            "Removing space between emoticons...\n",
            "Correcting spacing...\n",
            "Converting emoticons to tags...\n",
            "Substituting final paranthesis with tags...\n",
            "Converting numbers to tags...\n",
            "Converting hashtags to tags...\n",
            "Converting repetitions of symbols to tags...\n",
            "Converting elongated words to tags...\n",
            "Converting to lowercase...\n",
            "Correcting spacing...\n",
            "Updating vocabulary...\n",
            "Splitting data in train and test set...\n",
            "Converting data...\n",
            "['<hashtag> boston $ <number> for a standard commuter <number> laptop bag from rickshaw bagworks ( $ <number> value the right bag c . <repeat> <url> <hashtag> groupon'\n",
            " 'hey <user> follow me please it means so much for me <number>'\n",
            " \"<user> haha they love my beauty skills don't they ! ahaa . it hasn't failed we never spend money ! carly , chels , seanie + sammi , come !\"\n",
            " ... 'leg and body cramp'\n",
            " '<user> heyy follow me back please . <repeat> it would make my day if you hit my follow buttom ( <heart> i know you will ! ^ ^ #'\n",
            " '<number>x<number> custom picture frame / poster frame <number> \" wide complete matte black frame ( fw<number>bk<number> this frame is manufact']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting data...\n",
            "['galco royal guard inside the pant holster for <number> <number> <number>/<number>- inch colt , para , springfield ( natural , left-hand the'\n",
            " '<user> yes , i have to talk to u again <neutralface> i did something . you gonna kill me like always jajajaja'\n",
            " '<user> alzheimers very moving apparently . will pick it up on iplayer .'\n",
            " ... 'cant <elong> wait for my pizzza to get here !'\n",
            " '<user> fraser or harris tomorrow ? shall we stay after school tuesday and wednesday ? and yeah same i saw her in plymouth ! ! !'\n",
            " 'so happy for my babygirl <user> for getting asked to prom yayayayay go patrick go !']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embedding matrix...\n",
            "Found 1193514 word vectors.\n",
            "Converted 1 words (0 misses)\n",
            "Building model...\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 120, 100)          43982400  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 200)               121200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44123801 (168.32 MB)\n",
            "Trainable params: 141401 (552.35 KB)\n",
            "Non-trainable params: 43982400 (167.78 MB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training the model...\n",
            "Epoch 1/10\n",
            "14179/14179 [==============================] - 253s 17ms/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.5047\n",
            "Epoch 2/10\n",
            "14179/14179 [==============================] - 240s 17ms/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.5047\n",
            "Epoch 3/10\n",
            "14179/14179 [==============================] - 241s 17ms/step - loss: 0.6931 - accuracy: 0.5030 - val_loss: 0.6931 - val_accuracy: 0.5047\n",
            "Epoch 4/10\n",
            "14179/14179 [==============================] - 236s 17ms/step - loss: 0.6931 - accuracy: 0.5028 - val_loss: 0.6931 - val_accuracy: 0.5047\n",
            "Epoch 5/10\n",
            "14179/14179 [==============================] - 240s 17ms/step - loss: 0.6931 - accuracy: 0.5030 - val_loss: 0.6931 - val_accuracy: 0.5047\n",
            "Epoch 6/10\n",
            " 1544/14179 [==>...........................] - ETA: 3:05 - loss: 0.6931 - accuracy: 0.5050"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3fdbf122d5ea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-4bab663d66be>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(weights_path, is_prep, train_preprocessed_path, test_preprocessed_path, submission_path, full_data)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     classifier.fit_predict(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-96c399c3f6ad>\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, Y, ids_test, X_test, prediction_path, batch_size, epochs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training the model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         self.__model.fit(X_train_pad, Y_train, batch_size, epochs,\n\u001b[0m\u001b[1;32m    240\u001b[0m                         validation_data=(X_val_pad, Y_val))\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "He4mCmjziQ54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oT4Kep5ai4Hr"
      }
    }
  ]
}