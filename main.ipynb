{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fc6882",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4964a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global path\n",
    "GLOBAL_PATH = \"C:/Users/rayji/OneDrive/Documents/Projet 2 ML\"\n",
    "\n",
    "# GloVe\n",
    "GLOVE_PATH = f\"{GLOBAL_PATH}/data/glove.twitter.27B.100d.txt\"\n",
    "\n",
    "# Train full\n",
    "TRAIN_NEG_FULL_PATH = f\"{GLOBAL_PATH}/data/train_neg_full.txt\"\n",
    "TRAIN_POS_FULL_PATH = f\"{GLOBAL_PATH}/data/train_pos_full.txt\"\n",
    "\n",
    "# Train\n",
    "TRAIN_NEG_PATH = f\"{GLOBAL_PATH}/data/train_neg.txt\"\n",
    "TRAIN_POS_PATH = f\"{GLOBAL_PATH}/data/train_pos.txt\"\n",
    "\n",
    "# Test\n",
    "TEST_PATH = f\"{GLOBAL_PATH}/data/test_data.txt\"\n",
    "\n",
    "# Preprocessed data\n",
    "TRAIN_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/train_gru.csv\"\n",
    "TEST_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/test_gru.csv\"\n",
    "\n",
    "# Weight\n",
    "WEIGHT_PATH = f\"{GLOBAL_PATH}/weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel(ABC):\n",
    "    def __init__(self, weights_path: str):\n",
    "        self.__weights_path = weights_path\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_preprocessing_methods(self, is_test: bool = False):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_predict(self, X, y, ids_test, X_test, prediction_path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, ids, X, path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_submission(ids: list[int], predictions: list[int], path: str):\n",
    "        # Generating the submission file\n",
    "        submission = pd.DataFrame(columns=[\"Id\", \"Prediction\"],\n",
    "                                data={\"Id\": ids, \"Prediction\": predictions})\n",
    "\n",
    "        # For many models the labels are 0 or 1. Replacing 0s with -1s.\n",
    "        submission[\"Prediction\"].replace(0, -1, inplace=True)\n",
    "\n",
    "        # Saving the file\n",
    "        submission.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_data(X: pd.DataFrame, y: pd.DataFrame, test_size: float = 0.2, random_state: int = 42, **kwargs) -> tuple:\n",
    "        print(\"Splitting data in train and test set...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, **kwargs)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/2.6 MB 825.8 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/2.6 MB 1.2 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.4/2.6 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.8/2.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.3/2.6 MB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.9/2.6 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
      "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
      "     ---------------------------------------- 0.0/57.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.2/57.2 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: editdistpy\n",
      "  Building wheel for editdistpy (pyproject.toml): started\n",
      "  Building wheel for editdistpy (pyproject.toml): finished with status 'error'\n",
      "Failed to build editdistpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for editdistpy (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      <string>:23: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'editdistpy.levenshtein' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for editdistpy\n",
      "ERROR: Could not build wheels for editdistpy, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell # Fuzzy search and word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk weights\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import preprocessing\n",
    "from preprocessing import *\n",
    "# Reload the library\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "    methods.extend([\n",
    "    'drop_duplicates',\n",
    "    'remove_parentheses',\n",
    "    'remove_parentheses',\n",
    "    'remove_parentheses',\n",
    "    'remove_tag',\n",
    "    'remove_selected_characters',\n",
    "    'correct_spacing_indexing',\n",
    "    'remove_space_between_emoticons',\n",
    "    'correct_spacing_indexing',\n",
    "    'hashtags_to_tags',\n",
    "    'correct_spacing_indexing',\n",
    "    'word_segmentation',\n",
    "    'correct_spacing_indexing',\n",
    "    'slang_to_word',\n",
    "    'correct_spacing_indexing',\n",
    "    'correct_spelling',\n",
    "    'lemmatize',\n",
    "    'remove_stopwords',\n",
    "    'numbers_to_tags',\n",
    "    'replace_entities_with_tags',\n",
    "    'correct_spacing_indexing'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(\n",
    "    train_preprocessed_path=TRAIN_PREP_PATH, test_preprocessed_path=TEST_PREP_PATH, full_data=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing methods according to the chosen classifier\n",
    "      on the train and test data\n",
    "\n",
    "    :param csr: chosen classifier (child of AbstractModel)\n",
    "    :type csr: AbstractModel\n",
    "    :param train_preprocessed_path: path to load train data\n",
    "    :type train_preprocessed_path: str\n",
    "    :param test_preprocessed_path: path to load test data\n",
    "    :type test_preprocessed_path: str\n",
    "    :param full_data: if False, the small dataset (200K rows) is used\n",
    "    :type full_data: bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data\n",
    "    if full_data:\n",
    "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
    "    else:\n",
    "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
    "\n",
    "    train_preprocessing = Preprocessing(dataset_files, is_test=False)\n",
    "    test_preprocessing = Preprocessing([TEST_PATH], is_test=True)\n",
    "\n",
    "    # Preprocess it\n",
    "\n",
    "    methods = []\n",
    "\n",
    "    methods.extend([\n",
    "        'drop_duplicates',\n",
    "        'correct_spacing_indexing',\n",
    "        'remove_parentheses',\n",
    "        'correct_spacing_indexing',\n",
    "        'word_segmentation',\n",
    "        'correct_spacing_indexing',\n",
    "        'correct_spacing_indexing',\n",
    "        'remove_selected_characters',\n",
    "        'hashtags_to_tags',\n",
    "        'correct_spacing_indexing',\n",
    "        'lemmatize',\n",
    "        ])\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(train_preprocessing, method)()\n",
    "\n",
    "    methods.append('drop_duplicates')\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(test_preprocessing, method)()\n",
    "\n",
    "    train_df = train_preprocessing.__get__()\n",
    "    #train_df.to_csv(train_preprocessed_path, index=False)\n",
    "\n",
    "    test_df = test_preprocessing.__get__()\n",
    "    #test_df.to_csv(test_preprocessed_path, index=False)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "073db7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `drop_duplicates`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `word_segmentation`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181321/181321 [00:35<00:00, 5140.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_selected_characters`\n",
      "Removing selected characters...\n",
      "Executing: `hashtags_to_tags`\n",
      "Converting hashtags to tags...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `lemmatize`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181321/181321 [02:15<00:00, 1335.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `drop_duplicates`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `word_segmentation`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2799.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_selected_characters`\n",
      "Removing selected characters...\n",
      "Executing: `hashtags_to_tags`\n",
      "Converting hashtags to tags...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `lemmatize`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1513.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `drop_duplicates`\n",
      "Executing: `__get__`\n",
      "Executing: `__get__`\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed_dataset, test_preprocessed_dataset = run_preprocessing(full_data=False)\n",
    "train_preprocessed_dataset.to_csv('C:/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/data/preprocessed/train_preprocessed.csv', index=False)\n",
    "test_preprocessed_dataset.to_csv('C:/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/data/preprocessed/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0c752973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7910657658899766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_preprocessed_dataset['text'], train_preprocessed_dataset['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=100000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b5c6f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 3847, '.': 3108, '(': 1954, 'firstname': 1314, '-': 778, ')': 361, 'fuck': 208, 'cd': 169, 'xxx': 140, 'hama': 137, 'fucking': 113, 'shit': 79, 'friday': 73, 'pc': 63, 'pm': 62, 'anymore': 47, 'eth': 46, 'hardcover': 45, 'int': 42, 'boyfriend': 40, 'proud': 39, 'okay': 38, 'hmm': 37, 'mb': 36, 'kid': 35, 'facebook': 35, 'saturday': 32, 'justin': 29, 'hang': 28, 'co': 27, 'april': 25, 'english': 25, 'fax': 24, 'der': 23, 'american': 23, 'monday': 23, 'sunday': 21, 'email': 21, 'chelsea': 21, 'girlfriend': 20, 'blog': 20, 'online': 20, 'inc': 19, 'joel': 19, 'oof': 17, 'thursday': 17, 'liam': 17, 'gb': 17, 'mah': 16, 'favourite': 16, 'youtube': 16, 'etc': 16, 'spanish': 15, 'def': 14, 'philippine': 14, 'sch': 13, 'fave': 13, 'girls': 13, 'tuesday': 12, 'internet': 12, 'samsung': 12, 'est': 11, 'ord': 11, 'christian': 11, 'nokia': 11}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Download the English language dictionary\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "texts = test_preprocessed_dataset['text']\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove non-alphabetic characters (keep only letters, parentheses, numbers, '.', '-', and '!')\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9.!\\-()]', ' ', text)\n",
    "    # Tokenize\n",
    "    return word_tokenize(clean_text.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [clean_and_tokenize(text) for text in texts]\n",
    "flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "\n",
    "\n",
    "# Load a set of standard English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Filter out standard English words\n",
    "non_standard_tokens = [word for word in flat_tokens if word not in english_words]\n",
    "\n",
    "# Count the frequencies\n",
    "word_freq = Counter(non_standard_tokens)\n",
    "\n",
    "# You can define a threshold for what you consider 'high frequency'\n",
    "high_freq_threshold = 10  # Example threshold\n",
    "high_freq_words = {word: count for word, count in word_freq.items() if count > high_freq_threshold}\n",
    "\n",
    "ordered_words = {word: count for word, count in sorted(high_freq_words.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(ordered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8c943f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_ordered_words = pd.DataFrame.from_dict(ordered_words, orient='index', columns=['Count'])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df_ordered_words.to_csv('data/preprocessed/ordered_words.csv', index_label='Word')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41708fc-b3b0-4072-b90d-8b0346b36339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import utils \n",
    "from utils import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "# Reload the library\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_data()\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1154c8a4-4675-412d-a949-bda303e6ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the number of lines in each dataset\n",
    "print(data_train.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_pickle() to open the .pkl file\n",
    "#vocab = pd.read_pickle('vocab.pkl')\n",
    "#cooc_matrix = pd.read_pickle('cooc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cooc_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rayji\\OneDrive\\Documents\\GitHub\\ml-project-2-big-three\\main.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m col_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Access the element at the specified row and column indices\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m row \u001b[39m=\u001b[39m cooc_matrix\u001b[39m.\u001b[39mgetrow(row_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m element \u001b[39m=\u001b[39m row[\u001b[39m0\u001b[39m, col_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# 'element' now contains the value at the specified position in the sparse matrix\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cooc_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# Assuming 'sparse_matrix' is your sparse matrix\n",
    "# Replace row_index and col_index with the specific indices you want to access\n",
    "row_index = 0\n",
    "col_index = 0\n",
    "\n",
    "# Access the element at the specified row and column indices\n",
    "row = cooc_matrix.getrow(row_index)\n",
    "element = row[0, col_index]\n",
    "\n",
    "# 'element' now contains the value at the specified position in the sparse matrix\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>can't wait to fake tan tonight ! hate being pale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>&lt;user&gt; darling i lost my internet connection ....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>kanguru defender basic 4 gb usb 2.0 flash driv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>rizan is sad now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>no text back ? yea , he mad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       <user> i dunno justin read my mention or not ....      1\n",
       "1       because your logic is so dumb , i won't even c...      1\n",
       "2       \" <user> just put casper in a box ! \" looved t...      1\n",
       "3       <user> <user> thanks sir > > don't trip lil ma...      1\n",
       "4       visiting my brother tmr is the bestest birthda...      1\n",
       "...                                                   ...    ...\n",
       "199995   can't wait to fake tan tonight ! hate being pale      0\n",
       "199996  <user> darling i lost my internet connection ....      0\n",
       "199997  kanguru defender basic 4 gb usb 2.0 flash driv...      0\n",
       "199998                                   rizan is sad now      0\n",
       "199999                        no text back ? yea , he mad      0\n",
       "\n",
       "[200000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train['text'], data_train['label'], test_size=0.01, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(vectorizer.transform(test_data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_submission(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
