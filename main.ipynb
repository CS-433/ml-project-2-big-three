{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fc6882",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4964a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global path\n",
    "GLOBAL_PATH = \"C:/Users/rayji/OneDrive/Documents/Projet 2 ML\"\n",
    "\n",
    "# GloVe\n",
    "GLOVE_PATH = f\"{GLOBAL_PATH}/data/glove.twitter.27B.100d.txt\"\n",
    "\n",
    "# Train full\n",
    "TRAIN_NEG_FULL_PATH = f\"{GLOBAL_PATH}/data/train_neg_full.txt\"\n",
    "TRAIN_POS_FULL_PATH = f\"{GLOBAL_PATH}/data/train_pos_full.txt\"\n",
    "\n",
    "# Train\n",
    "TRAIN_NEG_PATH = f\"{GLOBAL_PATH}/data/train_neg.txt\"\n",
    "TRAIN_POS_PATH = f\"{GLOBAL_PATH}/data/train_pos.txt\"\n",
    "\n",
    "# Test\n",
    "TEST_PATH = f\"{GLOBAL_PATH}/data/test_data.txt\"\n",
    "\n",
    "# Preprocessed data\n",
    "TRAIN_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/train_gru.csv\"\n",
    "TEST_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/test_gru.csv\"\n",
    "\n",
    "# Weight\n",
    "WEIGHT_PATH = f\"{GLOBAL_PATH}/weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel(ABC):\n",
    "    def __init__(self, weights_path: str):\n",
    "        self.__weights_path = weights_path\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_preprocessing_methods(self, is_test: bool = False):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_predict(self, X, y, ids_test, X_test, prediction_path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, ids, X, path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_submission(ids: list[int], predictions: list[int], path: str):\n",
    "        # Generating the submission file\n",
    "        submission = pd.DataFrame(columns=[\"Id\", \"Prediction\"],\n",
    "                                data={\"Id\": ids, \"Prediction\": predictions})\n",
    "\n",
    "        # For many models the labels are 0 or 1. Replacing 0s with -1s.\n",
    "        submission[\"Prediction\"].replace(0, -1, inplace=True)\n",
    "\n",
    "        # Saving the file\n",
    "        submission.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_data(X: pd.DataFrame, y: pd.DataFrame, test_size: float = 0.2, random_state: int = 42, **kwargs) -> tuple:\n",
    "        print(\"Splitting data in train and test set...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, **kwargs)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/2.6 MB 825.8 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/2.6 MB 1.2 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.4/2.6 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.8/2.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.3/2.6 MB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.9/2.6 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
      "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
      "     ---------------------------------------- 0.0/57.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.2/57.2 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: editdistpy\n",
      "  Building wheel for editdistpy (pyproject.toml): started\n",
      "  Building wheel for editdistpy (pyproject.toml): finished with status 'error'\n",
      "Failed to build editdistpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for editdistpy (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      <string>:23: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'editdistpy.levenshtein' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for editdistpy\n",
      "ERROR: Could not build wheels for editdistpy, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell # Fuzzy search and word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTICONS_GLOVE = {\n",
    "  '<smile>': [':-]', '0:3', '8)', \":'-)\", '>^_^<', '(^_^)', \"(';')\", ':*',\n",
    "    '(^^)/', ':)', ':>', '(*_*)', '(^^)v', '=3', ':}', ';^)', ':->', '^_^;',\n",
    "    '=)', '(^o^)', '*)', '(^.^)', '^_^', '\\\\o/', '^5', '(__)', '(#^.^#)', '0:)',\n",
    "    '(^^)', ';]', ':-*', ':^)', ':3', '(+_+)', ';)', \":')\", '(:', ':-3', ':-}',\n",
    "    ';-)', ':-)', ':]', '*-)', 'o/\\\\o', '=]', '(^_-)', '8-)', ':o)', ':c)',\n",
    "    '(^_^)/', '(o.o)', ':o', '>:)', '8-0', ':-0', ';3', '>:3', '3:)', ':-o',\n",
    "    '}:)', 'o_0', '^^;', 'xx', 'xxx', '^o^', ':d', ' c:'],\n",
    "  '<lolface>': [':-p', ':p', ':b', ':-b', 'x-p', '=p'],\n",
    "  '<heart>': ['<3'],\n",
    "  '<neutralface>': ['=\\\\', '>:/', '(..)', '(._.)', ':-/', ':|', '>.<', ':-.',\n",
    "    \"('_')\", '=/', ':/', ':#', '(-_-)', 'o-o', 'o_o', ':$', '>:\\\\', ':@', ':-|',\n",
    "    '><>', '(-.-)', ':\\\\', '<+', ':-@'],\n",
    "  '<sadface>': [';(', '(~_~)', ':c', ':[', ':-&', ':(', '>:[', ':&', ':-c',\n",
    "    ';n;', \":'(\", ';;', ':-[', ';-;', '%)', ':<', '<\\\\3', ':{', ';_;', '=(',\n",
    "    'v.v', 'm(__)m', '</3', \":'-(\", ':-<']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk weights\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import PreprocessingUtils, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(\n",
    "    csr: AbstractModel, train_preprocessed_path=TRAIN_PREP_PATH, test_preprocessed_path=TEST_PREP_PATH, full_data=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing methods according to the chosen classifier\n",
    "      on the train and test data\n",
    "\n",
    "    :param csr: chosen classifier (child of AbstractModel)\n",
    "    :type csr: AbstractModel\n",
    "    :param train_preprocessed_path: path to load train data\n",
    "    :type train_preprocessed_path: str\n",
    "    :param test_preprocessed_path: path to load test data\n",
    "    :type test_preprocessed_path: str\n",
    "    :param full_data: if False, the small dataset (200K rows) is used\n",
    "    :type full_data: bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data\n",
    "    if full_data:\n",
    "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
    "    else:\n",
    "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
    "\n",
    "    train_preprocessing = Preprocessing(dataset_files, submission=False)\n",
    "    test_preprocessing = Preprocessing([TEST_PATH], submission=True)\n",
    "\n",
    "    # Preprocess it\n",
    "    for method in csr.get_preprocessing_methods(istest=False):\n",
    "        getattr(train_preprocessing, method)()\n",
    "\n",
    "    for method in csr.get_preprocessing_methods(istest=True):\n",
    "        getattr(test_preprocessing, method)()\n",
    "\n",
    "    # Save it\n",
    "    train_df = train_preprocessing.get()\n",
    "    train_df = train_df.sample(frac=1)\n",
    "\n",
    "    train_df.to_csv(train_preprocessed_path, index=False)\n",
    "    test_preprocessing.get().to_csv(test_preprocessed_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(\n",
    "    csr: AbstractModel, train_preprocessed_path=TRAIN_PREP_PATH, test_preprocessed_path=TEST_PREP_PATH, full_data=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing methods according to the chosen classifier\n",
    "      on the train and test data\n",
    "\n",
    "    :param csr: chosen classifier (child of AbstractModel)\n",
    "    :type csr: AbstractModel\n",
    "    :param train_preprocessed_path: path to load train data\n",
    "    :type train_preprocessed_path: str\n",
    "    :param test_preprocessed_path: path to load test data\n",
    "    :type test_preprocessed_path: str\n",
    "    :param full_data: if False, the small dataset (200K rows) is used\n",
    "    :type full_data: bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data\n",
    "    if full_data:\n",
    "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
    "    else:\n",
    "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
    "\n",
    "    train_preprocessing = Preprocessing(dataset_files, submission=False)\n",
    "    test_preprocessing = Preprocessing([TEST_PATH], submission=True)\n",
    "\n",
    "    # Preprocess it\n",
    "\n",
    "    methods = []\n",
    "\n",
    "    methods.extend([\n",
    "    'remove_endings',\n",
    "    'correct_spacing_indexing',\n",
    "    'remove_space_between_emoticons',\n",
    "    'correct_spacing_indexing',\n",
    "    'emoticons_to_tags',\n",
    "    'final_parenthesis_to_tags',\n",
    "    'numbers_to_tags',\n",
    "    'hashtags_to_tags',\n",
    "    'repeat_to_tags',\n",
    "    'elongs_to_tags',\n",
    "    'to_lower',\n",
    "    'correct_spacing_indexing'\n",
    "    ])\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(train_preprocessing, method)()\n",
    "\n",
    "    methods.append('drop_duplicates')\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(test_preprocessing, method)()\n",
    "\n",
    "    # Save it\n",
    "    train_df = train_preprocessing.get()\n",
    "    train_df = train_df.sample(frac=1)\n",
    "\n",
    "    train_df.to_csv(train_preprocessed_path, index=False)\n",
    "    test_preprocessing.get().to_csv(test_preprocessed_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41708fc-b3b0-4072-b90d-8b0346b36339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import utils \n",
    "from utils import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "# Reload the library\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_data()\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1154c8a4-4675-412d-a949-bda303e6ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the number of lines in each dataset\n",
    "print(data_train.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_pickle() to open the .pkl file\n",
    "#vocab = pd.read_pickle('vocab.pkl')\n",
    "#cooc_matrix = pd.read_pickle('cooc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cooc_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rayji\\OneDrive\\Documents\\GitHub\\ml-project-2-big-three\\main.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m col_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Access the element at the specified row and column indices\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m row \u001b[39m=\u001b[39m cooc_matrix\u001b[39m.\u001b[39mgetrow(row_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m element \u001b[39m=\u001b[39m row[\u001b[39m0\u001b[39m, col_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# 'element' now contains the value at the specified position in the sparse matrix\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cooc_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# Assuming 'sparse_matrix' is your sparse matrix\n",
    "# Replace row_index and col_index with the specific indices you want to access\n",
    "row_index = 0\n",
    "col_index = 0\n",
    "\n",
    "# Access the element at the specified row and column indices\n",
    "row = cooc_matrix.getrow(row_index)\n",
    "element = row[0, col_index]\n",
    "\n",
    "# 'element' now contains the value at the specified position in the sparse matrix\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>can't wait to fake tan tonight ! hate being pale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>&lt;user&gt; darling i lost my internet connection ....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>kanguru defender basic 4 gb usb 2.0 flash driv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>rizan is sad now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>no text back ? yea , he mad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       <user> i dunno justin read my mention or not ....      1\n",
       "1       because your logic is so dumb , i won't even c...      1\n",
       "2       \" <user> just put casper in a box ! \" looved t...      1\n",
       "3       <user> <user> thanks sir > > don't trip lil ma...      1\n",
       "4       visiting my brother tmr is the bestest birthda...      1\n",
       "...                                                   ...    ...\n",
       "199995   can't wait to fake tan tonight ! hate being pale      0\n",
       "199996  <user> darling i lost my internet connection ....      0\n",
       "199997  kanguru defender basic 4 gb usb 2.0 flash driv...      0\n",
       "199998                                   rizan is sad now      0\n",
       "199999                        no text back ? yea , he mad      0\n",
       "\n",
       "[200000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train['text'], data_train['label'], test_size=0.01, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(vectorizer.transform(test_data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_submission(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
