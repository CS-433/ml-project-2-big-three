{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fc6882",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4964a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global path\n",
    "GLOBAL_PATH = \"C:/Users/rayji/OneDrive/Documents/Projet 2 ML\"\n",
    "\n",
    "# GloVe\n",
    "GLOVE_PATH = f\"{GLOBAL_PATH}/data/glove.twitter.27B.100d.txt\"\n",
    "\n",
    "# Train full\n",
    "TRAIN_NEG_FULL_PATH = f\"{GLOBAL_PATH}/data/train_neg_full.txt\"\n",
    "TRAIN_POS_FULL_PATH = f\"{GLOBAL_PATH}/data/train_pos_full.txt\"\n",
    "\n",
    "# Train\n",
    "TRAIN_NEG_PATH = f\"{GLOBAL_PATH}/data/train_neg.txt\"\n",
    "TRAIN_POS_PATH = f\"{GLOBAL_PATH}/data/train_pos.txt\"\n",
    "\n",
    "# Test\n",
    "TEST_PATH = f\"{GLOBAL_PATH}/data/test_data.txt\"\n",
    "\n",
    "# Preprocessed data\n",
    "TRAIN_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/train_gru.csv\"\n",
    "TEST_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/test_gru.csv\"\n",
    "\n",
    "# Weight\n",
    "WEIGHT_PATH = f\"{GLOBAL_PATH}/weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel(ABC):\n",
    "    def __init__(self, weights_path: str):\n",
    "        self.__weights_path = weights_path\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_preprocessing_methods(self, is_test: bool = False):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_predict(self, X, y, ids_test, X_test, prediction_path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, ids, X, path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_submission(ids: list[int], predictions: list[int], path: str):\n",
    "        # Generating the submission file\n",
    "        submission = pd.DataFrame(columns=[\"Id\", \"Prediction\"],\n",
    "                                data={\"Id\": ids, \"Prediction\": predictions})\n",
    "\n",
    "        # For many models the labels are 0 or 1. Replacing 0s with -1s.\n",
    "        submission[\"Prediction\"].replace(0, -1, inplace=True)\n",
    "\n",
    "        # Saving the file\n",
    "        submission.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_data(X: pd.DataFrame, y: pd.DataFrame, test_size: float = 0.2, random_state: int = 42, **kwargs) -> tuple:\n",
    "        print(\"Splitting data in train and test set...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, **kwargs)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/2.6 MB 825.8 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/2.6 MB 1.2 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.4/2.6 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.8/2.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.3/2.6 MB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.9/2.6 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
      "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
      "     ---------------------------------------- 0.0/57.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.2/57.2 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: editdistpy\n",
      "  Building wheel for editdistpy (pyproject.toml): started\n",
      "  Building wheel for editdistpy (pyproject.toml): finished with status 'error'\n",
      "Failed to build editdistpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for editdistpy (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      <string>:23: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'editdistpy.levenshtein' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for editdistpy\n",
      "ERROR: Could not build wheels for editdistpy, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell # Fuzzy search and word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk weights\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import preprocessing\n",
    "from preprocessing import *\n",
    "# Reload the library\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(\n",
    "    csr: AbstractModel, train_preprocessed_path=TRAIN_PREP_PATH, test_preprocessed_path=TEST_PREP_PATH, full_data=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing methods according to the chosen classifier\n",
    "      on the train and test data\n",
    "\n",
    "    :param csr: chosen classifier (child of AbstractModel)\n",
    "    :type csr: AbstractModel\n",
    "    :param train_preprocessed_path: path to load train data\n",
    "    :type train_preprocessed_path: str\n",
    "    :param test_preprocessed_path: path to load test data\n",
    "    :type test_preprocessed_path: str\n",
    "    :param full_data: if False, the small dataset (200K rows) is used\n",
    "    :type full_data: bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data\n",
    "    if full_data:\n",
    "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
    "    else:\n",
    "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
    "\n",
    "    train_preprocessing = Preprocessing(dataset_files, submission=False)\n",
    "    test_preprocessing = Preprocessing([TEST_PATH], submission=True)\n",
    "\n",
    "    # Preprocess it\n",
    "    for method in csr.get_preprocessing_methods(istest=False):\n",
    "        getattr(train_preprocessing, method)()\n",
    "\n",
    "    for method in csr.get_preprocessing_methods(istest=True):\n",
    "        getattr(test_preprocessing, method)()\n",
    "\n",
    "    # Save it\n",
    "    train_df = train_preprocessing.get()\n",
    "    train_df = train_df.sample(frac=1)\n",
    "\n",
    "    train_df.to_csv(train_preprocessed_path, index=False)\n",
    "    test_preprocessing.get().to_csv(test_preprocessed_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(\n",
    "    train_preprocessed_path=TRAIN_PREP_PATH, test_preprocessed_path=TEST_PREP_PATH, full_data=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing methods according to the chosen classifier\n",
    "      on the train and test data\n",
    "\n",
    "    :param csr: chosen classifier (child of AbstractModel)\n",
    "    :type csr: AbstractModel\n",
    "    :param train_preprocessed_path: path to load train data\n",
    "    :type train_preprocessed_path: str\n",
    "    :param test_preprocessed_path: path to load test data\n",
    "    :type test_preprocessed_path: str\n",
    "    :param full_data: if False, the small dataset (200K rows) is used\n",
    "    :type full_data: bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data\n",
    "    if full_data:\n",
    "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
    "    else:\n",
    "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
    "\n",
    "    train_preprocessing = Preprocessing(dataset_files, is_test=False)\n",
    "    test_preprocessing = Preprocessing([TEST_PATH], is_test=True)\n",
    "\n",
    "    # Preprocess it\n",
    "\n",
    "    methods = []\n",
    "\n",
    "    methods.extend([\n",
    "    'drop_duplicates',\n",
    "    'remove_parentheses',\n",
    "    'remove_parentheses',\n",
    "    'remove_parentheses',\n",
    "    'remove_selected_characters',\n",
    "    'remove_tag',\n",
    "    'correct_spacing_indexing',\n",
    "    'remove_space_between_emoticons',\n",
    "    'correct_spacing_indexing',\n",
    "    'hashtags_to_tags',\n",
    "    'slang_to_word',\n",
    "    'word_segmentation',\n",
    "    'correct_spelling',\n",
    "    'lemmatize',\n",
    "    'remove_stopwords',\n",
    "    'correct_spacing_indexing'\n",
    "    ])\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(train_preprocessing, method)()\n",
    "\n",
    "    methods.append('drop_duplicates')\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(test_preprocessing, method)()\n",
    "\n",
    "    train_df = train_preprocessing.__get__()\n",
    "    #train_df.to_csv(train_preprocessed_path, index=False)\n",
    "\n",
    "    test_df = test_preprocessing.__get__()\n",
    "    #test_df.to_csv(test_preprocessed_path, index=False)\n",
    "\n",
    "    return train_df, test_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "073db7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `drop_duplicates`\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `remove_selected_characters`\n",
      "Removing selected characters...\n",
      "Executing: `remove_tag`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_space_between_emoticons`\n",
      "Removing space between emoticons...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `hashtags_to_tags`\n",
      "Converting hashtags to tags...\n",
      "Executing: `slang_to_word`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181321/181321 [00:00<00:00, 479377.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `word_segmentation`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 85172/181321 [08:00<09:34, 167.45it/s]"
     ]
    }
   ],
   "source": [
    "train_preprocessed_dataset, test_preprocessed_dataset = run_preprocessing(full_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "512a48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_dataset.to_csv('C:/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/data/preprocessed/train_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5c6f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 18372, 'urls': 11089, 'xxx': 5315, 'hama': 4465, 'll': 3968, 'fuck': 3024, 'cd': 2247, 've': 2148, 'okay': 2081, 'fucking': 1834, 'shit': 1545, 'pm': 1457, 'kid': 1197, 'hardcover': 1091, 'friday': 1015, 'pc': 971, 'int': 852, 'anymore': 839, 'boyfriend': 790, 'favourite': 782, 'sarah': 733, 'laptop': 690, 'hmm': 664, 'email': 595, 'proud': 589, 'saturday': 586, 'liam': 583, 'co': 534, 'american': 517, 'yamaha': 517, 'facebook': 512, 'goodnight': 508, 'justin': 504, 'english': 504, 'ipod': 479, 'mini': 466, 'bro': 465, 'hang': 453, 'oof': 451, 'prom': 445, 'eth': 437, 'der': 404, 'online': 397, 'nigga': 391, 'monday': 381, 'sunday': 376, 'kinda': 374, 'london': 364, 'thursday': 364, 'blog': 356, 'etc': 340, 'congrats': 338, 'girlfriend': 333, 'ohio': 315, 'ii': 315, 'iphone': 314, 'manuf': 306, 'thurs': 301, 'april': 296, 'jus': 290, 'french': 277, 'compaq': 273, 'hun': 261, 'louis': 245, 'fax': 244, 'misc': 244, 'america': 244, 'john': 240, 'nah': 236, 'internet': 236, 'app': 232, 'inst': 229, 'spanish': 227, 'chelsea': 227, 'others': 227, 'demo': 227, 'mph': 225, 'yougetmajorpointsif': 222, 'yup': 221, 'mama': 220, 'tom': 219, 'jan': 218, 'hows': 215, 'youtube': 214, 'ipad': 213, 'wednesday': 212, 'comm': 209, 'sony': 208, 'tuesday': 207, 'dec': 204, 'chris': 196, 'uni': 195, 'aah': 195, 'bio': 194, 'download': 194, 'cont': 193, 'outta': 193, 'dat': 190, 'lexer': 184, 'bos': 183, 'multi': 183, 'boys': 183, 'los': 183, 'jesus': 180, 'shes': 177, 'max': 176, 'allah': 172, 'iii': 167, 'eur': 166, 'texting': 164, 'james': 162, 'indonesia': 161, 'google': 161, 'gutted': 159, 'taylor': 156, 'paul': 156, 'chinese': 155, 'japanese': 153, 'england': 153, 'amazon': 152, 'australia': 152, 'samsung': 152, 'fri': 151, 'heard': 149, 'david': 148, 'ord': 148, 'adam': 145, 'philippine': 145, 'kalmia': 144, 'info': 144, 'michael': 142, 'goodbye': 142, 'earlier': 142, 'fiona': 141, 'ooh': 141, 'thoughtsduringschool': 141, 'anytime': 140, 'def': 138, 'realise': 138, 'bible': 137, 'com': 137, 'dunno': 137, 'thru': 133, 'mfg': 132, 'bpm': 131, 'alex': 129, 'theatre': 128, 'france': 128, 'christmas': 127, 'steamroll': 127, 'jul': 126, 'disney': 125, 'july': 123, 'skype': 123, 'rom': 122, 'bored': 122, 'tv': 122, 'pas': 121, 'website': 121, 'admins': 121, 'chicago': 120, 'mr': 120, 'inc': 120, 'matt': 119, 'est': 118, 'acc': 117, 'mod': 116, 'toshiba': 116, 'spain': 115, 'asian': 115, 'logo': 115, 'java': 115, 'barca': 114, 'meh': 113, 'blu': 113, 'italy': 113, 'magoo': 112, 'mcdonald': 111, 'selena': 111, 'shitty': 110, 'charlie': 110, 'waystomakemehappy': 110, 'upload': 109, 'thu': 109, 'lang': 107, 'italian': 106, 'fac': 106, 'christian': 106, 'software': 105, 'timeline': 105, 'florida': 105, 'miami': 105, 'european': 104, 'apr': 104, 'jessica': 104, 'asia': 103, 'india': 103, 'lem': 103, 'george': 103, 'fol': 102, 'cml': 102, 'texted': 101, 'texas': 101, 'panasonic': 100, 'diff': 100, 'brazilian': 100, 'fave': 100, 'paris': 98, 'microsoft': 97, 'scared': 96, 'med': 96, 'lao': 95, 'iowa': 94, 'offs': 93, 'mega': 92, 'ryan': 92, 'africa': 91, 'micros': 91, 'ups': 91, 'irish': 90, 'california': 90, 'org': 89, 'jewellery': 89, 'makeup': 89, 'british': 89, 'comp': 89, 'fab': 89, 'korean': 88, 'homie': 87, 'exp': 87, 'lb': 87, 'daniel': 87, 'oops': 87, 'playoff': 86, 'ella': 86, 'mann': 86, 'dena': 85, 'amp': 85, 'yum': 85, 'feb': 84, 'madrid': 83, 'ashley': 83, 'thomas': 83, 'ada': 82, 'centre': 82, 'austin': 82, 'playing': 81, 'sep': 81, 'canadian': 81, 'que': 81, 'gaga': 81, 'girls': 80, 'nov': 80, 'germany': 80, 'europe': 79, 'spam': 79, 'duh': 79, 'annie': 79, 'zzz': 79, 'itunes': 78, 'mexican': 78, 'robert': 78, 'bessie': 78, 'noah': 78, 'starbucks': 78, '8800mah': 77, 'cody': 77, 'tel': 77, 'cookie': 77, 'mia': 77, 'friends': 76, 'toronto': 76, 'utah': 75, 'hr': 75, 'jas': 75, 'manchester': 75, 'mauro': 75, 'sch': 75, 'scott': 75, 'skateboard': 74, 'lbs': 74, 'jones': 74, 'iraq': 74, 'nyx': 74, 'emo': 74, 'lovey': 74, 'nathan': 74, 'nike': 73, 'le': 73, 'kkt': 72, 'santa': 72, 'neighbour': 72, 'combo': 72, 'stu': 71, 'ireland': 71, 'washington': 71, 'indian': 71, 'guys': 71, 'bahama': 71, 'wifi': 71, 'christ': 70, 'opp': 70, 'leo': 69, 'mrs': 69, 'september': 69, 'eva': 68, 'ali': 68, 'desktop': 67, 'african': 67, 'danielle': 67, 'kevin': 66, 'ghz': 66, 'jim': 66, 'mhz': 66, 'hunter': 66, 'sydney': 66, 'pasta': 66, 'korea': 65, 'eff': 65, 'greek': 65, '4400mah': 64, 'preteen': 64, 'vhf': 64, 'camcorder': 64, 'sci': 64, 'tim': 64, 'freaking': 64, 'polly': 64, 'intel': 63, 'steve': 63, 'ltd': 63, 'porn': 63, 'jason': 63, 'mexico': 63, 'lauren': 62, 'netbook': 62, 'worldwide': 62, 'eng': 62, 'fwd': 62, 'bic': 62, 'thai': 62, 'jackson': 61, 'arsed': 61, 'kwh': 61, 'singapore': 61, 'wayne': 61, 'october': 61, 'cordless': 61, 'ian': 61, 'burger': 61, 'twi': 61, 'hannah': 60, 'offline': 60, 'rachel': 60, 'emily': 60, 'dammit': 60, 'lego': 60, 'eco': 60, 'bestselling': 59, 'aug': 59, 'poo': 59, 'yama': 59, 'dvrs': 58, 'greece': 58, 'cambridge': 58, 'sushi': 58, 'richard': 58, 'backpack': 58, 'andy': 58, 'dem': 58, 'taco': 58, 'vega': 58, 'hollywood': 57, 'brian': 57, 'gigabyte': 57, 'zealand': 57, 'una': 57, 'medicated': 56, 'mgr': 56, 'bullshit': 56, 'rugby': 56, 'countdown': 56, 'sahara': 56, 'latin': 55, 'johnny': 55, 'liverpool': 55, 'phil': 55, 'houston': 55, 'phillips': 55, 'oxford': 55, 'lisa': 55, 'tyler': 55, 'rumour': 55, 'nerd': 54, 'francisco': 54, 'dallas': 54, 'cunt': 54, 'kate': 54, 'shh': 54, 'kong': 54, 'wheres': 53, 'edward': 53, 'mariana': 53, 'nokia': 53, 'csc': 53, 'malaga': 53, 'sweden': 53, 'barbie': 52, 'nikon': 52, 'nanook': 52, 'pfc': 52, 'philip': 52, 'del': 52, 'brandon': 52, 'nav': 52, 'allen': 51, 'waal': 51, 'publ': 51, 'babes': 51, 'sims': 51, 'dave': 51, 'danny': 51, 'smiley': 50, 'mic': 50, 'hype': 50, 'yous': 50, 'supp': 50, 'jamie': 50, 'william': 50, 'katie': 50, 'lou': 50, 'lola': 50, 'okie': 50, 'mam': 49, 'simon': 49, 'ssh': 49, 'apps': 49, 'hawaii': 49, 'andrew': 49, 'celtic': 48, 'reuters': 48, 'exams': 48, 'megan': 48, 'obama': 48, '6600mah': 48, 'scotland': 48, 'cher': 48, 'cali': 48, 'sean': 48, 'ifindthatattractive': 48, 'boxes': 47, 'texts': 47, 'jakarta': 47, 'madison': 47, 'bella': 47, 'motorola': 47, 'jessie': 47, 'pkg': 47, 'karen': 47, 'lala': 47, 'hangover': 46, 'epson': 46, 'atlanta': 46, 'atm': 46, 'chloe': 46, 'roman': 46, 'username': 46, 'planning': 46, 'aaron': 46, 'argentina': 46, 'leeds': 46, 'vibe': 46, 'fucker': 46, 'enron': 46, 'kayla': 46, 'arizona': 45, 'cafe': 45, 'britain': 45, 'dur': 45, 'irma': 45, 'honour': 45, 'yankee': 45, 'gary': 44, 'lex': 44, 'teddy': 44, 'wii': 44, 'legs': 44, 'chem': 44, 'euro': 44, 'ebay': 44, 'claire': 44, 'ina': 44, 'yoko': 44, 'ab': 43, 'intro': 43, 'johnson': 43, 'ron': 43, 'menus': 43, 'mtg': 43, 'anne': 43, 'bags': 43, 'joseph': 43, 'hoo': 43, 'nicole': 42, 'promo': 42, 'nikki': 42, 'waterloo': 42, 'practise': 42, 'fella': 42, 'ole': 42, 'kari': 42, 'sana': 42, 'kendall': 42, 'hes': 42, 'cologne': 42, 'anthony': 41, 'colourful': 41, 'jon': 41, 'remastered': 41, 'elena': 41, 'omaha': 41, 'milan': 41, 'whens': 41, 'raining': 41, 'mickey': 41, 'disneyland': 41, 'alice': 41, 'celeb': 41, 'williams': 41, 'newcastle': 41, 'jennifer': 41, 'australian': 41, 'keith': 41, 'bros': 40, 'charles': 40, 'georgia': 40, 'dixon': 40, 'craig': 40, 'inbox': 40, 'argus': 40, 'wanted': 40, 'usu': 40, 'sta': 39, 'momus': 39, 'angeles': 39, 'indonesian': 39, 'admin': 39, 'hiya': 39, 'russian': 39, 'weirdo': 39, 'insp': 38, 'mario': 38, 'carolina': 38, 'bueno': 38, 'cutest': 38, 'filipino': 38, 'mayan': 38, 'julia': 38, 'swedish': 38, 'sleepover': 38, 'yougetmajorpoints': 38, 'dbl': 37, 'bayern': 37, 'lloyd': 37, 'lesbian': 37, 'alexis': 37, 'wilson': 37, 'str': 37, 'marley': 37, 'blk': 37, 'ninja': 37, 'cimon': 37, 'playlist': 37, 'niggas': 37, 'gotcha': 37, 'melissa': 36, 'esp': 36, 'jacob': 36, 'oke': 36, 'katy': 36, 'denver': 36, 'hugh': 36, 'surprised': 36, 'dialler': 36, 'charlotte': 36, 'teenager': 36, 'veggie': 36, 'dpi': 36, 'portuguese': 35, 'egypt': 35, 'soundtrack': 35, 'amanda': 35, 'chan': 35, 'bbl': 35, 'rome': 35, '10thingsiwanttohappen': 35, 'virginia': 35, 'eau': 35, 'nazi': 35, 'adidas': 35, 'loner': 34, 'jeremy': 34, 'pakistan': 34, 'sweets': 34, 'sem': 34, 'zen': 34, 'kenya': 34, 'arg': 34, 'december': 34, 'christina': 34, 'signing': 34, 'malaysia': 34, 'kane': 34, 'payne': 34, 'howard': 34, 'hon': 34, 'vancouver': 33, 'dy': 33, 'diego': 33, 'philly': 33, 'turkish': 33, 'canuck': 33, 'seattle': 33, 'bruno': 33, 'maine': 33, 'avatar': 33, 'matthew': 33, 'aussie': 33, 'diana': 33, 'kris': 33, 'victoria': 33, 'lifestyle': 33, 'cray': 33, 'nina': 33, 'kidding': 33, 'updated': 33, 'arab': 32, 'lucas': 32, 'stanley': 32, 'ont': 32, 'xerox': 32, 'polaroid': 32, 'halloween': 32, 'damon': 32, 'rae': 32, 'logan': 32, 'dublin': 32, 'melbourne': 32, 'birmingham': 32, 'portugal': 32, 'betta': 32, 'wiki': 32, 'gomez': 32, 'packaging': 31, 'glasgow': 31, 'beatles': 31, 'evans': 31, 'czech': 31, 'ethernet': 31, 'cuba': 31, 'len': 31, 'january': 31, 'bluetooth': 31, 'mayo': 31, 'greg': 31, 'techno': 31, 'holland': 31, 'happens': 31, 'maths': 31, 'davis': 31, 'pleased': 31, 'ellen': 31, 'yogurt': 31, 'taif': 31, 'telly': 30, 'pissed': 30, 'programme': 30, 'webcam': 30, 'loved': 30, 'neva': 30, 'leigh': 30, 'jackie': 30, 'jesse': 30, 'israel': 30, 'palau': 30, 'salsa': 30, 'tampa': 30, 'ronald': 30, 'sara': 30, 'hor': 30, 'boogie': 30, 'olympics': 30, 'choc': 30, 'oct': 30, 'michelle': 30, 'coolest': 30, 'std': 30, 'brisbane': 30, 'nara': 30, 'au': 29, 'illustrated': 29, 'november': 29, 'fr': 29, 'muslim': 29, 'thailand': 29, 'elizabeth': 29, 'levi': 29, 'ricky': 29, 'sim': 29, 'condo': 29, 'healthcare': 29, 'tina': 29, 'bryan': 29, 'incl': 29, 'cheryl': 29, 'calgary': 29, 'nevers': 29, 'psf': 29, 'poland': 29, 'lib': 29, 'addictive': 29, 'audi': 29, 'marcus': 29, 'pokemon': 29, 'kara': 29, 'init': 29, 'coursework': 29, 'derek': 29, 'dd': 28, 'hast': 28, 'buddha': 28, 'cdr': 28, 'marie': 28, 'stephen': 28, 'sophie': 28, 'cameron': 28, 'jewish': 28, 'casa': 28, 'mazda': 28, 'tvs': 28, 'andrea': 28, 'cardio': 28, 'lara': 28, 'babysitting': 28, 'peru': 28, 'kobe': 28, 'mari': 28, 'nero': 28, 'guardi': 28, 'booboo': 28, 'gogh': 28, 'kathy': 28, 'simpson': 28, 'harrypotterchatuplines': 28, 'brittany': 27, 'faux': 27, 'cassette': 27, 'crappy': 27, 'alexander': 27, 'chr': 27, 'brings': 27, 'sings': 27, 'sgd': 27, 'liao': 27, 'ergo': 27, 'carlos': 27, 'olympus': 27, 'hungarian': 27, 'montreal': 27, 'juan': 27, 'dora': 27, 'scottish': 27, 'miranda': 27, 'bruce': 27, 'devon': 27, 'doggie': 27, 'carly': 27, 'wendy': 27, 'favour': 27, 'raul': 27, 'arthur': 27, 'ppm': 27, 'doh': 27, 'blogger': 27, 'stan': 27, 'adele': 27, 'agatha': 27, 'antonio': 27, 'dubai': 27, 'youcangetmajorpointsif': 27, 'doug': 26, 'sheffield': 26, 'var': 26, 'lawson': 26, 'ter': 26, 'donut': 26, 'harris': 26, 'hologram': 26, 'alicia': 26, 'mosh': 26, 'perl': 26, 'disco': 26, 'harley': 26, 'ottawa': 26, 'podcast': 26, 'alana': 26, 'selma': 26, 'whf': 26, 'penn': 26, 'philadelphia': 26, 'rosa': 26, 'fart': 26, 'brighton': 26, 'dylan': 26, 'nintendo': 26, 'belgium': 26, 'modem': 25, 'meg': 25, 'lynn': 25, 'narnia': 25, 'pt': 25, 'grandad': 25, 'ops': 25, 'titan': 25, 'superstar': 25, 'orlando': 25, 'metro': 25, 'versa': 25, 'decor': 25, 'earbuds': 25, 'jour': 25, 'mai': 25, 'creme': 25, 'cuppa': 25, 'byte': 25, 'aston': 25, 'whitney': 25, 'mara': 25, 'arbour': 25, 'lilly': 25, 'seiko': 25, 'delilah': 25, 'martini': 25, 'dts': 25, 'pittsburgh': 25, 'olivia': 25, 'denmark': 25, 'offence': 24, 'fred': 24, 'norway': 24, 'vic': 24, 'windows': 24, 'iran': 24, 'smoothie': 24, 'natalie': 24, 'anderson': 24, 'alaska': 24, 'joel': 24, 'linda': 24, 'rutledge': 24, 'susan': 24, 'alyssa': 24, 'abba': 24, 'torres': 24, 'stefan': 24, 'payphone': 24, 'trans': 24, 'yuk': 24, 'phillip': 24, 'malawi': 24, 'eleanor': 24, 'liz': 24, 'gemini': 24, 'norton': 24, 'zach': 24, 'jew': 24, 'hal': 24, 'condom': 24, 'aunty': 24, 'tyr': 24, 'darwin': 24, 'homies': 24, 'pcs': 23, 'hamilton': 23, 'asa': 23, 'toni': 23, 'expo': 23, 'abby': 23, 'lincoln': 23, 'annoyed': 23, 'neighbourhood': 23, 'dir': 23, 'missed': 23, 'tokyo': 23, 'publishing': 23, 'britney': 23, 'mercury': 23, 'ira': 23, 'psycho': 23, 'nom': 23, 'perrin': 23, 'taurus': 23, 'shaun': 23, 'ste': 23, 'flo': 23, 'gtd': 23, 'shorty': 23, 'retro': 23, 'casio': 23, 'russell': 23, 'shane': 23, 'hugo': 23, 'helen': 23, 'neil': 23, 'vegas': 23, 'fer': 23, 'kama': 23, 'puerto': 23, 'playstation': 23, 'helena': 23, 'howell': 23, 'netflix': 23, 'lima': 23, 'salem': 23, 'slovakia': 23, 'switzerland': 23, 'rechargeable': 22, 'stressed': 22, 'walmart': 22, 'roland': 22, 'capri': 22, 'trending': 22, 'maggie': 22, 'sandra': 22, 'julie': 22, 'att': 22, 'freestyle': 22, 'espresso': 22, 'ghana': 22, 'bestseller': 22, 'vac': 22, 'meatball': 22, 'reminds': 22, 'yrs': 22, 'smartphone': 22, 'khz': 22, 'jeez': 22, 'acclaimed': 22, 'artwork': 22, 'rec': 22, 'fucked': 22, 'robs': 22, 'essex': 22, 'photos': 22, 'brooke': 22, 'kennedy': 22, 'herr': 22, 'elise': 22, 'princeton': 22, 'halifax': 22, 'sian': 22, 'abs': 22, 'tues': 22, 'shawn': 22, 'ellie': 22, 'patrick': 22, 'tia': 22, 'ned': 22, 'shrek': 22, 'yong': 22, 'pizz': 22, 'vince': 21, 'programming': 21, 'syria': 21, 'gawd': 21, 'jamaica': 21, 'ard': 21, 'dedicated': 21, 'auckland': 21, 'sweatshirt': 21, 'fridge': 21, 'widescreen': 21, 'biblical': 21, 'leah': 21, 'instal': 21, 'delivers': 21, 'gucci': 21, 'frigging': 21, 'skint': 21, 'rumba': 21, 'eddie': 21, 'nashville': 21, 'database': 21, 'cardiff': 21, 'dayton': 21, 'ftp': 21, 'milkshake': 21, 'dhaka': 21, 'sasha': 21, 'islam': 21, 'klein': 21, 'donald': 21, 'ciao': 21, 'stevie': 21, 'marvin': 21, 'nissan': 21, 'jensen': 21, 'detroit': 21, 'haynes': 21, 'blogging': 21, 'celebs': 21, 'allie': 21, 'indiana': 21, 'bahamas': 21, 'wiley': 20, 'aaliyah': 20, 'jedi': 20, 'elvis': 20, '5200mah': 20, 'obj': 20, 'quran': 20, 'laos': 20, 'monica': 20, 'columbia': 20, 'hindi': 20, 'oscar': 20, 'pitt': 20, 'francis': 20, 'odyssey': 20, 'youknowwhatannoysme': 20, 'heres': 20, 'tailgate': 20, 'bristol': 20, 'shootout': 20, 'erin': 20, 'orleans': 20, 'texan': 20, 'adrian': 20, 'hayes': 20, 'dix': 20, 'february': 20, 'brighter': 20, 'biggie': 20, 'dryer': 19, 'postcard': 19, 'thompson': 19, 'roy': 19, 'meow': 19, 'fm': 19, 'barnes': 19, 'casey': 19, 'ibiza': 19, 'sanchez': 19, 'swede': 19, 'camden': 19, 'multimedia': 19, 'minolta': 19, 'bahrain': 19, 'tex': 19, 'va': 19, 'andre': 19, 'buying': 19, 'dennis': 19, 'kansa': 19, 'pennsylvania': 19, 'relaxing': 19, 'calvin': 19, 'ms': 19, 'takeaway': 19, 'jonathan': 19, 'hershey': 19, 'jonas': 19, 'cinderella': 19, 'ska': 19, 'vietnam': 19, 'xxi': 19, 'handheld': 19, 'manhattan': 19, 'truro': 19, 'eli': 19, 'chilli': 19, 'belfast': 19, '7800mah': 19, 'jose': 19, 'cleveland': 19, 'tomorrows': 19, 'hardcore': 19, 'sorta': 19, 'vanessa': 19, 'perth': 19, 'blunts': 19, 'alta': 19, 'linux': 19, 'judy': 18, 'networking': 18, 'apologise': 18, 'malay': 18, 'jill': 18, 'kook': 18, '74060': 18, 'alabama': 18, 'avery': 18, 'todd': 18, 'meanie': 18, 'shakespeare': 18, 'kingston': 18, 'xmas': 18, 'eden': 18, 'gigabit': 18, 'jamaican': 18, 'newbie': 18, 'hae': 18, 'courtney': 18, '130ash138': 18, 'frustrating': 18, 'caleb': 18, 'hols': 18, 'hudson': 18, 'tito': 18, 'samuel': 18, 'droid': 18, 'haley': 18, 'reggae': 18, 'startup': 18, 'moore': 18, 'blaine': 18, 'klaus': 18, 'hebron': 18, 'armband': 18, 'hollie': 18, 'moustache': 18, 'weblog': 18, 'chanel': 18, 'dlr': 18, 'leila': 18, 'aida': 18, 'barbara': 18, 'frankie': 18, 'som': 18, 'zap': 18, 'nate': 18, 'lunchtime': 18, 'abr': 18, 'eminem': 18, 'nathaniel': 18, 'girlies': 18, 'thewantedonthevoice': 18, 'kuna': 18, 'eos': 17, 'mimi': 17, 'viz': 17, 'missouri': 17, 'leslie': 17, 'douglas': 17, 'wichita': 17, 'knackered': 17, 'ono': 17, 'loyola': 17, 'zoe': 17, 'getty': 17, 'hutchinson': 17, 'sunni': 17, 'res': 17, 'pentax': 17, 'amsterdam': 17, 'adj': 17, 'alaric': 17, 'stat': 17, 'watson': 17, 'hungover': 17, 'delhi': 17, 'regina': 17, '76039': 17, 'paediatric': 17, 'microfiber': 17, 'hometown': 17, 'unix': 17, 'processing': 17, 'kidd': 17, 'stephanie': 17, 'vegan': 17, 'albert': 17, 'olympic': 17, 'reagan': 17, 'mccain': 17, 'granola': 17, 'asks': 17, 'cactus': 17, 'munich': 17, 'mercedes': 17, 'bonnie': 17, 'hitler': 17, 'seoul': 17, 'analog': 17, 'darren': 17, 'wei': 17, 'nikita': 17, 'debbie': 17, 'hipster': 17, 'dana': 17, 'savannah': 17, 'monroe': 17, 'ob': 17, 'tomlin': 17, 'veg': 16, 'deli': 16, 'kab': 16, '17096': 16, 'exc': 16, 'gracie': 16, 'boutique': 16, 'prem': 16, 'dreams': 16, 'caribbean': 16, 'tennessee': 16, 'comoro': 16, 'indie': 16, 'vito': 16, 'neville': 16, 'wisconsin': 16, 'mckay': 16, 'oreo': 16, '4500mah': 16, 'adv': 16, 'bi': 16, 'duracell': 16, 'toyota': 16, 'longtime': 16, 'hrs': 16, 'campbell': 16, 'bart': 16, 'faggot': 16, 'judd': 16, 'verizon': 16, 'piercings': 16, 'youknowicarewhen': 16, 'jetway': 16, 'nigeria': 16, 'baltimore': 16, 'worlds': 16, 'sunscreen': 16, 'morrison': 16, 'arr': 16, 'faves': 16, 'alfredo': 16, 'scr': 16, 'malang': 16, 'geez': 16, 'spamming': 16, 'christopher': 16, 'venice': 16, 'dork': 16, 'effie': 16, 'rico': 16, 'arabic': 16, 'lakes': 16, 'saba': 16, 'holmes': 16, 'joshua': 16, 'kurt': 16, 'lucia': 16, 'wal': 16, 'louisville': 16, 'maui': 16, 'haiti': 16, 'kush': 16, 'hottie': 16, 'trevor': 15, 'bradley': 15, 'elmo': 15, 'henderson': 15, 'winnie': 15, '26917': 15, 'ecu': 15, 'macintosh': 15, 'oakland': 15, 'marshall': 15, 'firstborn': 15, 'tanya': 15, 'val': 15, 'milf': 15, 'checkout': 15, 'nigerian': 15, 'pixel': 15, 'hahahahahahaha': 15, 'yukon': 15, 'blond': 15, 'aus': 15, 'moi': 15, 'latte': 15, 'netherlands': 15, 'omar': 15, 'fairytale': 15, '74966': 15, '26009': 15, 'rosie': 15, 'vienna': 15, 'madonna': 15, 'hilary': 15, 'jules': 15, 'rms': 15, 'reveals': 15, 'etta': 15, 'vicky': 15, 'approx': 15, 'barron': 15, 'backyard': 15, 'cruz': 15, 'foods': 15, 'becky': 15, 'harvard': 15, 'minneapolis': 15, 'yorkshire': 15, 'isaac': 15, 'hooray': 15, '74023': 15, 'raleigh': 15, 'bitchy': 15, 'edinburgh': 15, 'nightlight': 15, 'gavin': 15, 'fujitsu': 15, 'hebrew': 15, 'noticed': 15, 'oregon': 15, 'darcy': 15, 'unisex': 15, 'mitchell': 15, 'bianca': 15, 'neh': 15, 'malaysian': 15, 'brooklyn': 15, 'luis': 15, 'danish': 15, 'lotta': 15, 'beaut': 15, 'aux': 15, 'idaho': 15, 'dino': 15, 'pumped': 15, 'cheetos': 15, 'scorpio': 15, '17015': 14, 'sega': 14, 'steele': 14, '74093': 14, 'gallo': 14, 'auth': 14, 'reno': 14, 'khalid': 14, 'ikea': 14, 'jodi': 14, 'gaul': 14, '50036': 14, 'oklahoma': 14, 'caesar': 14, 'paracetamol': 14, 'inv': 14, 'mobil': 14, 'joan': 14, 'cory': 14, 'econ': 14, 'colombia': 14, 'blvd': 14, 'xvi': 14, 'dakota': 14, 'hispanic': 14, 'libby': 14, '26025': 14, 'rem': 14, 'contains': 14, 'songwriter': 14, 'castro': 14, 'exec': 14, 'freeware': 14, '29ash138': 14, 'maryland': 14, 'cuties': 14, 'capitol': 14, 'samantha': 14, 'minnesota': 14, 'shannon': 14, 'ayala': 14, 'victorian': 14, 'burgundy': 14, 'ashton': 14, 'shattered': 14, 'richmond': 14, 'prev': 14, 'oliver': 14, 'qty': 14, 'mkt': 14, 'midterm': 14, 'fatwa': 14, 'essen': 14, 'yikes': 14, 'como': 14, 'ollie': 14, 'walkman': 14, 'oulu': 14, 'nebraska': 14, 'britt': 14, 'munchkin': 14, 'noe': 14, 'berber': 14, 'carrie': 14, 'litre': 14, 'durham': 14, 'herman': 14, 'nadia': 14, 'luther': 14, 'bryant': 14, 'randoms': 14, '58396lt': 14, 'hungary': 14, 'marshmallow': 14, 'omani': 14, 'persian': 14, 'gran': 14, 'cuzco': 14, 'yves': 14, 'hughes': 14, 'motherfucker': 14, 'videos': 14, 'oms': 14, 'naomi': 14, 'yippee': 14, 'orwell': 14, 'powell': 13, 'kanji': 13, 'waldo': 13, 'hindu': 13, 'overpowered': 13, 'presley': 13, 'esther': 13, 'manufacturing': 13, 'ctr': 13, 'polypropylene': 13, 'gatorade': 13, 'barack': 13, 'closing': 13, 'customize': 13, '17060': 13, 'macau': 13, 'freebie': 13, 'stuart': 13, '26963': 13, 'roswell': 13, 'strep': 13, 'sid': 13, 'chen': 13, 'portland': 13, 'pennington': 13, '26021': 13, 'chrysler': 13, 'loudspeaker': 13, 'grammy': 13, 'coordinator': 13, 'pres': 13, 'wabash': 13, 'abe': 13, '90ash138': 13, 'nigel': 13, 'mecca': 13, 'smurfs': 13, 'kenny': 13, 'somethings': 13, 'songs': 13, 'miguel': 13, 'woodstock': 13, 'milwaukee': 13, 'igbo': 13, 'romeo': 13, 'capricorn': 13, 'elli': 13, '5481225': 13, 'kami': 13, 'textured': 13, 'trojan': 13, 'cairo': 13, 'clinton': 13, 'afghan': 13, 'lizzie': 13, 'beep': 13, 'hess': 13, 'tsp': 13, 'velcro': 13, 'buddhism': 13, 'spartan': 13, 'nos': 13, '17273': 13, 'graz': 13, 'joeys': 13, 'ethan': 13, 'qatar': 13, 'cen': 13, 'dianna': 13, 'rollin': 13, '41290': 13, 'nottingham': 13, 'flashback': 13, 'clara': 13, 'starving': 13, 'bridget': 13, 'bradford': 13, 'lopez': 13, 'hayden': 13, 'unattractivethingsaboutme': 13, 'ajax': 13, 'karina': 13, 'lindsay': 13, 'freddie': 13, 'ito': 13, 'quinn': 13, 'karl': 13, '200ash105': 13, 'incipit': 13, 'lamar': 13, 'memphis': 13, 'jer': 13, 'jello': 13, 'quebec': 13, 'mich': 13, 'thor': 13, 'amhara': 13, 'nicene': 13, 'felix': 13, 'anita': 13, 'vii': 13, 'cyprus': 13, 'cindy': 13, 'ificanthaveyou': 13, 'brianna': 13, 'samoa': 13, 'tagalog': 13, 'nighty': 13, 'cuter': 13, 'virgo': 13, 'jaime': 13, 'marion': 12, 'voicemail': 12, 'avon': 12, 'breathtaking': 12, 'gupta': 12, 'owen': 12, '74273': 12, 'midi': 12, '26267': 12, 'pr': 12, 'reid': 12, 'hilton': 12, 'ed': 12, 'bombay': 12, 'integrated': 12, 'modelling': 12, 'veda': 12, 'broadband': 12, 'goth': 12, 'danang': 12, 'approved': 12, 'pepsi': 12, 'episodes': 12, '74068': 12, 'armani': 12, 'curated': 12, 'terra': 12, 'julian': 12, 'diabolo': 12, 'athens': 12, 'trina': 12, 'royce': 12, 'lanka': 12, '90210': 12, 'kristy': 12, 'colombian': 12, 'akita': 12, 'pulitzer': 12, 'jigsaw': 12, 'apollo': 12, 'bol': 12, 'ferrari': 12, 'ellis': 12, 'garcia': 12, 'garry': 12, '26966': 12, 'rooftop': 12, 'croatia': 12, '521138': 12, 'smithsonian': 12, 'bette': 12, 'fps': 12, 'felicia': 12, 'beebe': 12, 'hwy': 12, 'mohawk': 12, 'cara': 12, 'hamas': 12, 'kristen': 12, 'photog': 12, 'gillette': 12, 'hitachi': 12, 'nelly': 12, 'pinot': 12, 'newport': 12, 'wmk': 12, 'nolan': 12, 'airline': 12, '440138': 12, 'catherine': 12, 'tillman': 12, '74267': 12, 'edgar': 12, 'maury': 12, 'lebanon': 12, 'afghanistan': 12, 'ontario': 12, '26093': 12, 'evan': 12, 'archie': 12, 'nauru': 12, 'proc': 12, 'univ': 12, 'sudan': 12, 'watts': 12, 'eph': 12, 'murray': 12, 'thewantedep4days': 12, 'fulfil': 12, 'munchies': 12, 'isi': 12, 'aguilar': 12, 'asch': 12, 'gabriel': 12, 'lana': 12, 'kenneth': 12, 'paycheck': 12, 'teresa': 12, 'wheelchair': 12, 'ivan': 12, 'lineup': 12, 'nicer': 12, 'natasha': 12, 'licence': 12, 'marilyn': 12, 'cert': 12, '74066': 12, 'lawrence': 12, 'norwich': 12, 'kentucky': 12, 'blackpool': 12, 'gerard': 12, 'carmen': 12, 'sharon': 12, 'simone': 12, 'ringtone': 12, 'michele': 12, 'havana': 12, 'pisces': 12, 'sanskrit': 12, 'tristan': 12, 'adelaide': 12, 'gina': 12, 'sonia': 12, 'organise': 12, 'hoping': 12, 'appreciated': 12, 'jeffrey': 11, 'marvellous': 11, 'amiga': 11, 'lindsey': 11, 'sweatpants': 11, 'ultrasound': 11, 'robinson': 11, 'sebastian': 11, 'ritz': 11, 'ericsson': 11, 'panga': 11, 'aud': 11, 'maxi': 11, 'floyd': 11, 'portsmouth': 11, 'rte': 11, 'marsala': 11, '50005': 11, 'announces': 11, 'kyoto': 11, 'dolce': 11, 'trademark': 11, 'leon': 11, 'aloha': 11, 'swansea': 11, '17041': 11, '26020': 11, 'allows': 11, 'paige': 11, 'asama': 11, 'acct': 11, 'gordon': 11, 'mckee': 11, 'ola': 11, 'baddy': 11, 'bantu': 11, 'crawford': 11, 'levine': 11, '262ash138': 11, 'phys': 11, 'nanak': 11, 'hogwarts': 11, 'vinci': 11, 'emmy': 11, 'katrina': 11, 'isabel': 11, 'wok': 11, 'stinky': 11, 'casper': 11, 'stacy': 11, 'wannabe': 11, 'caitlin': 11, 'nairobi': 11, 'pluto': 11, 'payback': 11, 'sgt': 11, 'juliet': 11, 'karachi': 11, 'guam': 11, 'arkansas': 11, 'joyce': 11, 'kansas': 11, 'islamic': 11, 'jefferson': 11, 'stewart': 11, 'malta': 11, 'diane': 11, 'patented': 11, 'finn': 11, 'supra': 11, 'micah': 11, 'cuisinart': 11, 'carey': 11, '1669751': 11, 'alton': 11, 'gibson': 11, 'ct': 11, 'briefcase': 11, '26919': 11, 'martha': 11, 'coughing': 11, '200ashbk': 11, 'bollywood': 11, 'nassau': 11, 'erica': 11, 'marx': 11, 'saudi': 11, 'makeover': 11, 'carlo': 11, 'bubba': 11, 'uber2012': 11, 'sens': 11, 'nevada': 11, 'athena': 11, 'pogo': 11, 'bernard': 11, 'lisbon': 11, '586138': 11, 'saul': 11, 'shortie': 11, 'studying': 11, '130ashbk': 11, 'failed': 11, 'romanian': 11, 'gangsta': 11, 'jav': 11, 'finnish': 11, 'impressed': 11, 'replaces': 11, 'amos': 11, 'geog': 11, 'windsor': 11, 'israeli': 11, 'obs': 11, 'dillon': 11, 'erik': 11, 'marin': 11, 'louise': 11, 'myles': 11, 'yuri': 11, 'carson': 11, 'ely': 11, 'btu': 11, 'barf': 11, 'thorpe': 11, 'bling': 11, 'dunn': 11, 'ems': 11, 'padilla': 11, 'alfred': 11, 'audrey': 11, 'ironic': 11, 'kart': 11, 'newman': 11, 'papua': 11, 'graffiti': 11, 'hyde': 11, 'metallica': 11, 'harvey': 11, 'ukraine': 11, 'vida': 11, 'hawaiian': 11, 'joking': 11, 'names': 11, 'couture': 11, 'prov': 11, 'percy': 11, 'globelumia800': 11, 'lamps': 11, 'psst': 11, 'happybirthdaypresley': 11}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Download the English language dictionary\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "texts = train_preprocessed_dataset['text']\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove non-alphabetic characters (keep only letters, parentheses, numbers, '.', '-', and '!')\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9.!\\-()]', ' ', text)\n",
    "    # Tokenize\n",
    "    return word_tokenize(clean_text.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [clean_and_tokenize(text) for text in texts]\n",
    "flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "\n",
    "\n",
    "# Load a set of standard English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Filter out standard English words\n",
    "non_standard_tokens = [word for word in flat_tokens if word not in english_words]\n",
    "\n",
    "# Count the frequencies\n",
    "word_freq = Counter(non_standard_tokens)\n",
    "\n",
    "# You can define a threshold for what you consider 'high frequency'\n",
    "high_freq_threshold = 10  # Example threshold\n",
    "high_freq_words = {word: count for word, count in word_freq.items() if count > high_freq_threshold}\n",
    "\n",
    "ordered_words = {word: count for word, count in sorted(high_freq_words.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(ordered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41708fc-b3b0-4072-b90d-8b0346b36339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import utils \n",
    "from utils import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "# Reload the library\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_data()\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1154c8a4-4675-412d-a949-bda303e6ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the number of lines in each dataset\n",
    "print(data_train.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_pickle() to open the .pkl file\n",
    "#vocab = pd.read_pickle('vocab.pkl')\n",
    "#cooc_matrix = pd.read_pickle('cooc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cooc_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rayji\\OneDrive\\Documents\\GitHub\\ml-project-2-big-three\\main.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m col_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Access the element at the specified row and column indices\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m row \u001b[39m=\u001b[39m cooc_matrix\u001b[39m.\u001b[39mgetrow(row_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m element \u001b[39m=\u001b[39m row[\u001b[39m0\u001b[39m, col_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/main.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# 'element' now contains the value at the specified position in the sparse matrix\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cooc_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# Assuming 'sparse_matrix' is your sparse matrix\n",
    "# Replace row_index and col_index with the specific indices you want to access\n",
    "row_index = 0\n",
    "col_index = 0\n",
    "\n",
    "# Access the element at the specified row and column indices\n",
    "row = cooc_matrix.getrow(row_index)\n",
    "element = row[0, col_index]\n",
    "\n",
    "# 'element' now contains the value at the specified position in the sparse matrix\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>can't wait to fake tan tonight ! hate being pale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>&lt;user&gt; darling i lost my internet connection ....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>kanguru defender basic 4 gb usb 2.0 flash driv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>rizan is sad now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>no text back ? yea , he mad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       <user> i dunno justin read my mention or not ....      1\n",
       "1       because your logic is so dumb , i won't even c...      1\n",
       "2       \" <user> just put casper in a box ! \" looved t...      1\n",
       "3       <user> <user> thanks sir > > don't trip lil ma...      1\n",
       "4       visiting my brother tmr is the bestest birthda...      1\n",
       "...                                                   ...    ...\n",
       "199995   can't wait to fake tan tonight ! hate being pale      0\n",
       "199996  <user> darling i lost my internet connection ....      0\n",
       "199997  kanguru defender basic 4 gb usb 2.0 flash driv...      0\n",
       "199998                                   rizan is sad now      0\n",
       "199999                        no text back ? yea , he mad      0\n",
       "\n",
       "[200000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train['text'], data_train['label'], test_size=0.01, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(vectorizer.transform(test_data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_submission(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
