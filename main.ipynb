{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell # Fuzzy search and word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d3afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\utils.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils \n",
    "from utils import *\n",
    "import preprocessing\n",
    "from preprocessing import *\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nltk weights\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30849f1",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global path\n",
    "GLOBAL_PATH = \"C:/Users/rayji/OneDrive/Documents/Projet 2 ML\"\n",
    "\n",
    "# GloVe\n",
    "GLOVE_PATH = f\"{GLOBAL_PATH}/data/glove.twitter.27B.100d.txt\"\n",
    "\n",
    "# Train full\n",
    "TRAIN_NEG_FULL_PATH = f\"{GLOBAL_PATH}/data/train_neg_full.txt\"\n",
    "TRAIN_POS_FULL_PATH = f\"{GLOBAL_PATH}/data/train_pos_full.txt\"\n",
    "\n",
    "# Train\n",
    "TRAIN_NEG_PATH = f\"{GLOBAL_PATH}/data/train_neg.txt\"\n",
    "TRAIN_POS_PATH = f\"{GLOBAL_PATH}/data/train_pos.txt\"\n",
    "\n",
    "# Test\n",
    "TEST_PATH = f\"{GLOBAL_PATH}/data/test_data.txt\"\n",
    "\n",
    "# Preprocessed data\n",
    "TRAIN_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/train_gru.csv\"\n",
    "TEST_PREP_PATH = f\"{GLOBAL_PATH}/data/preprocessed/test_gru.csv\"\n",
    "\n",
    "# Weight\n",
    "WEIGHT_PATH = f\"{GLOBAL_PATH}/weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff7b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: symspellpy in c:\\users\\rayji\\anaconda3\\envs\\transformers\\lib\\site-packages (6.7.7)\n",
      "Requirement already satisfied: editdistpy>=0.1.3 in c:\\users\\rayji\\anaconda3\\envs\\transformers\\lib\\site-packages (from symspellpy) (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4133d9c",
   "metadata": {},
   "source": [
    "# Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647cd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe2bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessed_full_dataset = load_preprocessed_data(os.getcwd() + \"/twitter-datasets/train_preprocessed_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d438ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tres or pack 6 ( difficulty 10 of 10 obj...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! !   thankful  ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season   were fu...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user  i could actually kill that girl i m so s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user   user   user  i find that very hard to b...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270477</th>\n",
       "      <td>a warning sign   (  rt  user  the negativity y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270478</th>\n",
       "      <td>user  ff too thank youuu ) )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270479</th>\n",
       "      <td>i just love shumpa ! that s my girl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270480</th>\n",
       "      <td>the best way to start a day ! no matter what h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270481</th>\n",
       "      <td>fr enc hie swan t1dtou i m not from french but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text label\n",
       "0        vinco tres or pack 6 ( difficulty 10 of 10 obj...    -1\n",
       "1        glad i dot have taks tomorrow ! !   thankful  ...    -1\n",
       "2        1-3 vs celtics in the regular season   were fu...    -1\n",
       "3        user  i could actually kill that girl i m so s...    -1\n",
       "4        user   user   user  i find that very hard to b...    -1\n",
       "...                                                    ...   ...\n",
       "2270477  a warning sign   (  rt  user  the negativity y...     1\n",
       "2270478                       user  ff too thank youuu ) )     1\n",
       "2270479                i just love shumpa ! that s my girl     1\n",
       "2270480  the best way to start a day ! no matter what h...     1\n",
       "2270481  fr enc hie swan t1dtou i m not from french but...     1\n",
       "\n",
       "[2270482 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocessed_full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49ee061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayji\\OneDrive\\Documents\\GitHub\\ml-project-2-big-three\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "print(current_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel(ABC):\n",
    "    def __init__(self, weights_path: str):\n",
    "        self.__weights_path = weights_path\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_preprocessing_methods(self, is_test: bool = False):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_predict(self, X, y, ids_test, X_test, prediction_path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, ids, X, path):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_submission(ids: list[int], predictions: list[int], path: str):\n",
    "        # Generating the submission file\n",
    "        submission = pd.DataFrame(columns=[\"Id\", \"Prediction\"],\n",
    "                                data={\"Id\": ids, \"Prediction\": predictions})\n",
    "\n",
    "        # For many models the labels are 0 or 1. Replacing 0s with -1s.\n",
    "        submission[\"Prediction\"].replace(0, -1, inplace=True)\n",
    "\n",
    "        # Saving the file\n",
    "        submission.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_data(X: pd.DataFrame, y: pd.DataFrame, test_size: float = 0.2, random_state: int = 42, **kwargs) -> tuple:\n",
    "        print(\"Splitting data in train and test set...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, **kwargs)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9a6c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: symspellpy in c:\\users\\rayji\\anaconda3\\envs\\transformers\\lib\\site-packages (6.7.7)\n",
      "Requirement already satisfied: editdistpy>=0.1.3 in c:\\users\\rayji\\anaconda3\\envs\\transformers\\lib\\site-packages (from symspellpy) (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell # Fuzzy search and word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk weights\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'c:\\\\Users\\\\rayji\\\\OneDrive\\\\Documents\\\\GitHub\\\\ml-project-2-big-three\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import preprocessing\n",
    "from preprocessing import *\n",
    "# Reload the library\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf71859",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    'drop_duplicates',\n",
    "    'correct_spacing_indexing',\n",
    "    'remove_parentheses',\n",
    "    'correct_spacing_indexing',\n",
    "    'word_segmentation',\n",
    "    'correct_spacing_indexing',\n",
    "    'correct_spacing_indexing',\n",
    "    'remove_selected_characters',\n",
    "    'hashtags_to_tags',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc584fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(method_list, full_data=True):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing methods according to the chosen classifier\n",
    "      on the train and test data\n",
    "\n",
    "    :param csr: chosen classifier (child of AbstractModel)\n",
    "    :type csr: AbstractModel\n",
    "    :param train_preprocessed_path: path to load train data\n",
    "    :type train_preprocessed_path: str\n",
    "    :param test_preprocessed_path: path to load test data\n",
    "    :type test_preprocessed_path: str\n",
    "    :param full_data: if False, the small dataset (200K rows) is used\n",
    "    :type full_data: bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data\n",
    "    if full_data:\n",
    "        dataset_files = [TRAIN_NEG_FULL_PATH, TRAIN_POS_FULL_PATH]\n",
    "    else:\n",
    "        dataset_files = [TRAIN_NEG_PATH, TRAIN_POS_PATH]\n",
    "\n",
    "    train_preprocessing = Preprocessing(dataset_files, is_test=False)\n",
    "    test_preprocessing = Preprocessing([TEST_PATH], is_test=True)\n",
    "\n",
    "    # Preprocess it\n",
    "\n",
    "    methods = method_list\n",
    "    \n",
    "    for method in methods:\n",
    "        getattr(test_preprocessing, method)()\n",
    "\n",
    "    methods.append('drop_duplicates')\n",
    "\n",
    "\n",
    "    for method in methods:\n",
    "        getattr(train_preprocessing, method)()\n",
    "\n",
    "    train_df = train_preprocessing.__get__()\n",
    "    #train_df.to_csv(train_preprocessed_path, index=False)\n",
    "\n",
    "    test_df = test_preprocessing.__get__()\n",
    "    #test_df.to_csv(test_preprocessed_path, index=False)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "073db7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `drop_duplicates`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `word_segmentation`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2270482/2270482 [05:15<00:00, 7194.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_selected_characters`\n",
      "Removing selected characters...\n",
      "Executing: `hashtags_to_tags`\n",
      "Converting hashtags to tags...\n",
      "Executing: `drop_duplicates`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_parentheses`\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `word_segmentation`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1800.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `correct_spacing_indexing`\n",
      "Correcting spacing...\n",
      "Executing: `remove_selected_characters`\n",
      "Removing selected characters...\n",
      "Executing: `hashtags_to_tags`\n",
      "Converting hashtags to tags...\n",
      "Executing: `drop_duplicates`\n",
      "Executing: `__get__`\n",
      "Executing: `__get__`\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed_dataset, test_preprocessed_dataset = run_preprocessing(full_data=True)\n",
    "train_preprocessed_dataset.to_csv('C:/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/data/preprocessed/train_preprocessed_full.csv', index=False)\n",
    "test_preprocessed_dataset.to_csv('C:/Users/rayji/OneDrive/Documents/GitHub/ml-project-2-big-three/data/preprocessed/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c752973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8106527900426561\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_preprocessed_dataset['text'], train_preprocessed_dataset['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=100000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c6f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rayji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 3856, '.': 3161, 'url': 2073, '...': 2062, '(': 1985, '-': 827, 'rt': 536, '..': 458, '3': 391, 'lol': 388, ')': 364, '2': 286, 'im': 259, 'haha': 252, '1': 173, '4': 159, 'xxx': 133, 'xx': 131, 'omg': 113, 'fuck': 103, '5': 96, 'guys': 94, 'friends': 87, '6': 83, 'ok': 79, 'okay': 79, 'shit': 75, '10': 74, '8': 73, 'girls': 70, 'dvd': 67, 'friday': 65, 'cd': 63, 'aww': 62, '20': 60, 'things': 59, 'lmao': 58, 'hahaha': 58, 'fans': 58, 'makes': 56, 'looks': 49, 'anymore': 47, '7': 47, 'tweets': 47, 'followers': 46, 'missed': 46, 'hours': 46, 'laptop': 46, 'hardcover': 45, 'wanted': 44, 'boys': 43, 'wants': 43, '12': 41, 'sooo': 41, 'awww': 41, 'sucks': 39, 'proud': 39, 'mom': 38, '18': 38, 'called': 38, 'followed': 36, 'hp': 36, 'women': 34, 'fucking': 34, 'soo': 34, 'tv': 33, '100': 33, 'saturday': 32, 'manuf': 32, 'years': 32, 'justin': 32, 'manufactured': 31, '30': 31, 'gets': 31, 'boyfriend': 31, 'lil': 29, 'hurts': 29, 'books': 29, 'cuz': 29, 'mini': 29, '1gb': 28, 'dm': 28, '50': 28, 'london': 28, 'hahah': 28, 'liam': 28, 'hehe': 27, 'ima': 27, 'playing': 26, 'ipod': 26, 'bored': 26, '16': 26, 'oz': 26, 'facebook': 26, 'feet': 25, 'niall': 25, 'bro': 25, 'english': 25, 'started': 25, 'sounds': 25, '11': 25, 'a-tech': 24, '9': 24, 'weeks': 24, 'ones': 24, '--': 24, 'loves': 24, 'bitches': 24, 'ed': 23, 'april': 23, 'oomf': 23, 'ee': 23, 'goodnight': 23, 'minutes': 23, 'ff': 23, 'sunday': 22, 'ddr': 22, '0': 22, 'texts': 22, 'months': 22, 'happened': 22, 'monday': 22, 'pics': 22, 'kid': 22, 'comments': 22, 'points': 21, 'words': 21, 'idk': 21, 'says': 21, 'means': 21, 'nt': 21, 'eyes': 21, 'chelsea': 21, '2012': 20, 'fav': 20, 'online': 20, 'lets': 20, 'feels': 19, '1d': 19, 'american': 19, 'ss': 19, 'prom': 19, 'scared': 19, '40': 19, 'joel': 19, 'disheartened': 19, 'nooo': 18, 'ahh': 18, 'yay': 18, 'dnt': 18, 'includes': 18, 'loved': 18, 'heard': 18, 'xd': 18, 'congrats': 18, 'kids': 18, 'parents': 18, 'inc': 18, 'goodbye': 18, 'starts': 18, '2nd': 17, 'yu': 17, '24': 17, 'compaq': 17, 'hun': 17, 'smh': 17, 'yy': 17, 'songs': 17, 'seems': 17, 'nigga': 17, 'zayn': 17, 'ii': 17, 'btw': 17, '1.25': 17, 'games': 17, 'lool': 16, 'using': 16, 'pc': 16, 'problems': 16, 'iphone': 16, 'blog': 16, 'co': 16, 'nd': 16, 'liked': 16, 'takes': 16, 'uk': 16, '17': 16, 'ohh': 16, 'asked': 16, 'thursday': 16, '15': 16, 'meee': 16, 'youtube': 16, 'mee': 16, 'usa': 16, 'children': 16, 'lmfao': 16, '420': 15, '500': 15, 'girlfriend': 15, 'james': 15, 'hands': 15, '21': 15, 'misc': 15, '1.5': 15, 'australia': 15, 'def': 15, 'favourite': 15, '70': 15, 'ive': 15, 'loool': 15, 'plz': 15, 'multi': 15, 'spanish': 15, 'mia': 15, 'yesss': 14, 'features': 14, 'ch': 14, '25': 14, 'thoughts': 14, 'rs': 14, 'exams': 14, '32': 14, 'skins': 14, 'earlier': 14, 'lines': 14, 'noticed': 14, 'ml': 14, 'shes': 13, '75': 13, 'usb': 13, 'kinda': 13, 'tt': 13, 'xo': 13, 'sch': 13, 'hang': 13, 'italy': 13, 'fits': 13, 'dreams': 13, 'mr': 13, 'jk': 13, 'shoes': 13, '33': 13, 'bt': 13, 'mins': 13, 'tears': 13, 'noo': 13, 'goin': 13, 'alot': 13, '23': 13, 'ahhh': 13, 'fml': 13, 'died': 13, '19': 13, 'pictures': 13, 'hd': 13, 'nokia': 13, 'movies': 12, 'app': 12, 'letting': 12, 'youu': 12, 'jesus': 12, 'tuesday': 12, '3rd': 12, 'happens': 12, 'moved': 12, 'talked': 12, 'bieber': 12, 'staying': 12, 'follows': 12, 'needed': 12, 'ages': 12, 'jus': 12, 'philippines': 12, 'pls': 12, 'knows': 12, '13': 12, 'john': 12, 'ahaha': 12, '256mb': 12, '101': 12, 'wtf': 12, 'lu': 12, 'samsung': 12, 'stories': 12, 'est': 11, 'america': 11, '200': 11, '000': 11, 'trending': 11, 'internet': 11, 'nathan': 11, '1st': 11, 'tickets': 11, '14': 11, 'tools': 11, 'gunna': 11, '60': 11, 'yall': 11, 'changed': 11, 'xoxo': 11, 'texas': 11, 'nfl': 11, 'personalized': 11, 'bags': 11, 'skinit': 11, '2009': 11, 'omaha': 11, 'christian': 11, '2010': 11, 'systems': 11, 'avi': 11, 'fr': 11, '2007': 11, 'tom': 11, 'acer': 11}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Download the English language dictionary\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "texts = test_preprocessed_dataset['text']\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove non-alphabetic characters (keep only letters, parentheses, numbers, '.', '-', and '!')\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9.!\\-()]', ' ', text)\n",
    "    # Tokenize\n",
    "    return word_tokenize(clean_text.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [clean_and_tokenize(text) for text in texts]\n",
    "flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "\n",
    "\n",
    "# Load a set of standard English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Filter out standard English words\n",
    "non_standard_tokens = [word for word in flat_tokens if word not in english_words]\n",
    "\n",
    "# Count the frequencies\n",
    "word_freq = Counter(non_standard_tokens)\n",
    "\n",
    "# You can define a threshold for what you consider 'high frequency'\n",
    "high_freq_threshold = 10  # Example threshold\n",
    "high_freq_words = {word: count for word, count in word_freq.items() if count > high_freq_threshold}\n",
    "\n",
    "ordered_words = {word: count for word, count in sorted(high_freq_words.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(ordered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8c943f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_ordered_words = pd.DataFrame.from_dict(ordered_words, orient='index', columns=['Count'])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df_ordered_words.to_csv('data/preprocessed/ordered_words.csv', index_label='Word')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
